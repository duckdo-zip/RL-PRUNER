{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXC93PvZHALw"
   },
   "source": [
    "# Import e path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "models_save_path = \"/workspace/RL-for-ViT/Models_mnist/\"\n",
    "results_save_path_agent = \"/workspace/RL-for-ViT/Results_mnist/\"\n",
    "dataset_path = \"/workspace/RL-for-ViT/Datasets_mnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries for data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for image manipulation and machine learning\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional\n",
    "import torch.optim as optim\n",
    "\n",
    "# Libraries for model evaluation metrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Libraries for tensor manipulation\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Libraries for simulation environments (gym)\n",
    "import gym\n",
    "\n",
    "# Libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Other common libraries\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(patches, temperature = 10000, dtype = torch.float32):\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "\n",
    "    y, x = torch.meshgrid(torch.arange(h, device = device), torch.arange(w, device = device), indexing = 'ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "    omega = torch.arange(dim // 4, device = device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim = 1)\n",
    "    return pe.type(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class SimpleAgentViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = []\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        *_, h, w, dtype = *img.shape, img.dtype\n",
    "\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        # Masking the input based on selected patches\n",
    "        mask = torch.tensor(self.patches, dtype=torch.bool)\n",
    "        x = x[:, mask, :]\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)\n",
    "\n",
    "    def set_patches(self, patch):\n",
    "        mean = torch.mean(patch)\n",
    "        selected = [0 if elem < mean else 1 for elem in patch]\n",
    "        self.patches = selected\n",
    "\n",
    "    def get_patches(self):\n",
    "        return self.patches\n",
    "\n",
    "    def get_att(self, img):\n",
    "        *_, h, w, dtype = *img.shape, img.dtype\n",
    "\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        attn, ff = self.transformer.layers[0]\n",
    "        x = attn(x)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "img_size = 28\n",
    "dataset_name = \"MNIST_BIG_\"\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.Resize(img_size),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "transform_validation = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
     ]
    }
   ],
   "source": [
    "# Prepare/download dataset\n",
    "trainset = torchvision.datasets.MNIST(root=dataset_path, train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "validationset = torchvision.datasets.MNIST(root=dataset_path, train=False, download=True, transform=transform_validation)\n",
    "\n",
    "dataset_size = len(validationset)\n",
    "validation_size = int(0.95 * dataset_size)\n",
    "test_size = dataset_size - validation_size\n",
    "\n",
    "validationset, testset = data.random_split(validationset, [validation_size, test_size])\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter_agent(model, optimizer, data, target):\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = functional.log_softmax(model(data), dim=1)\n",
    "    loss = functional.nll_loss(out, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    iteration_time = time.time() - start_time\n",
    "\n",
    "    return loss.item(), iteration_time\n",
    "\n",
    "# def evaluate_agent(model, data_load, device):\n",
    "\n",
    "#     patches = model.get_patches()\n",
    "#     model.set_patches(torch.tensor([1 for i in range(len(patches))], dtype=torch.float))\n",
    "#     model.eval()\n",
    "\n",
    "\n",
    "#     elements = 0\n",
    "#     csamp = 0\n",
    "#     tloss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in data_load:\n",
    "\n",
    "#             elements += len(data)\n",
    "#             data = data.to(device)\n",
    "#             target = target.to(device)\n",
    "\n",
    "#             output = functional.log_softmax(model(data), dim=1)\n",
    "#             loss = functional.nll_loss(output, target, reduction='sum')\n",
    "#             _, pred = torch.max(output, dim=1)\n",
    "\n",
    "#             tloss += loss.item()\n",
    "#             csamp += pred.eq(target).sum()\n",
    "\n",
    "#     loss_val = tloss / elements\n",
    "#     acc_val = (100.0 * csamp / elements).cpu()\n",
    "\n",
    "#     print('\\nAverage test loss: ' + '{:.4f}'.format(loss_val) +\n",
    "#           '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "#           '{:5}'.format(elements) + ' (' +\n",
    "#           '{:4.2f}'.format(acc_val) + '%)\\n')\n",
    "\n",
    "#     model.set_patches(patches)\n",
    "\n",
    "#     return loss_val, acc_val\n",
    "\n",
    "# def evaluate_agent(model, data_load, device, mode=False):\n",
    "\n",
    "#     model.eval()\n",
    "#     elements = 0\n",
    "#     csamp = 0\n",
    "#     tloss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in data_load:\n",
    "#             data = data.to(device)\n",
    "#             target = target.to(device)\n",
    "\n",
    "#             # mode에 따라 patch mask 선택 방식 다르게\n",
    "#             if mode == \"agent\":\n",
    "#                 # agent가 선택한 패치를 적용\n",
    "#                 state = env.get_state(data)\n",
    "#                 action = trainer.agent.select_action(state)\n",
    "#                 model.set_patches(action)\n",
    "\n",
    "#             elif mode == \"random\":\n",
    "#                 # 랜덤 마스크\n",
    "#                 action = env.action_space.sample()\n",
    "#                 model.set_patches(action)\n",
    "\n",
    "#             else:\n",
    "#                 # 모든 패치 사용\n",
    "#                 full_mask = torch.ones(len(model.get_patches()), dtype=torch.float32)\n",
    "#                 model.set_patches(full_mask)\n",
    "\n",
    "#             output = functional.log_softmax(model(data), dim=1)\n",
    "#             loss = functional.nll_loss(output, target, reduction='sum')\n",
    "#             _, pred = torch.max(output, dim=1)\n",
    "\n",
    "#             tloss += loss.item()\n",
    "#             csamp += pred.eq(target).sum()\n",
    "#             elements += len(data)\n",
    "\n",
    "#     loss_val = tloss / elements\n",
    "#     acc_val = (100.0 * csamp / elements).cpu()\n",
    "\n",
    "#     print(f\"\\nEval Mode: {mode}, Acc: {acc_val:.2f}%\")\n",
    "\n",
    "#     return loss_val, acc_val\n",
    "\n",
    "def evaluate_agent(model, agent, data_load, device, mode=None):\n",
    "    model.eval()\n",
    "\n",
    "    elements = 0\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "            elements += len(data)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # --- 패치 선택 모드 ---\n",
    "            if mode == \"agent\":\n",
    "                state = agent.env.get_state(data)\n",
    "                action = agent.select_action(state, eps=0.0)   # greedy 선택\n",
    "            elif mode == \"random\":\n",
    "                action = agent.env.action_space.sample()\n",
    "            else:\n",
    "                # 모든 패치 사용\n",
    "                action = torch.ones(len(agent.env.action_space.sample()), dtype=torch.float32)\n",
    "\n",
    "            model.set_patches(action)\n",
    "\n",
    "            # --- forward ---\n",
    "            output = F.log_softmax(model(data), dim=1)\n",
    "            loss = F.nll_loss(output, target, reduction=\"sum\")\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    loss_val = tloss / elements\n",
    "    acc_val = (100.0 * csamp / elements).cpu().item()\n",
    "\n",
    "    print(f\"\\nAccuracy ({mode}): {acc_val:.2f}%\")\n",
    "    return loss_val, acc_val\n",
    "\n",
    "def train_validation_agent(model, optimizer, train_data, train_target, validation_loader, device):\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  tr_loss = train_iter_agent(model, optimizer, train_data, train_target)\n",
    "\n",
    "  val_loss, val_acc = evaluate_agent(model, validation_loader, device)\n",
    "\n",
    "  iteration_time = time.time() - start_time\n",
    "\n",
    "  return tr_loss, val_loss, val_acc, iteration_time\n",
    "\n",
    " # Action space represented with a vector with one element for every feature\n",
    "class MultiContinue():\n",
    "\n",
    "    def __init__(self,  n_patch, device):\n",
    "        self.n_patch = n_patch\n",
    "        self.device = device\n",
    "\n",
    "    def sample(self):\n",
    "        action = np.random.rand(self.n_patch)\n",
    "        return torch.tensor(action, device=self.device, dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "class ViTEnv(gym.Env):\n",
    "    def __init__(self, ViTnet, n_patch, optimizer, loss_weight, time_weight, device, n_patch_selected = 1, seed = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ViTnet = ViTnet\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "        self.loss_weight = loss_weight\n",
    "        self.time_weight = time_weight\n",
    "\n",
    "        self.action_space = MultiContinue(n_patch, device)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_time_list = []\n",
    "\n",
    "    def step_train(self, action, train_data, train_target):\n",
    "\n",
    "        self.ViTnet.set_patches(action)\n",
    "\n",
    "        train_iter_agent(self.ViTnet, self.optimizer, train_data, train_target)\n",
    "\n",
    "\n",
    "    def step_reward(self, action, train_data, train_target):\n",
    "\n",
    "        self.ViTnet.set_patches(action)\n",
    "\n",
    "        action = ViTnet.get_patches()\n",
    "\n",
    "        print(f'  Patch list: {action}')\n",
    "        print(f'  Selected Patches: {action.count(1)}')\n",
    "\n",
    "        reward = self.get_reward(action, train_data = train_data, train_target = train_target)\n",
    "\n",
    "        return self.get_state(train_data), reward\n",
    "\n",
    "\n",
    "    def get_reward(self, action, train_data, train_target):\n",
    "\n",
    "        train_loss, iteration_time = train_iter_agent(self.ViTnet, self.optimizer, train_data, train_target)\n",
    "\n",
    "        self.train_time_list.append(iteration_time)\n",
    "        self.train_loss_list.append(train_loss)\n",
    "\n",
    "        num_zeros = action.count(0)\n",
    "        ideal_zeros = len(action) - n_patch_selected\n",
    "        patches_reward = (- abs(num_zeros - ideal_zeros) / ideal_zeros)\n",
    "        loss_reward = (self.train_loss_list[0]/train_loss)\n",
    "        print(f'  loss_reward: {loss_reward}')\n",
    "        print(f'  patches_reward: {patches_reward}')\n",
    "        reward = loss_reward * self.loss_weight + patches_reward * self.time_weight\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_state(self, data):\n",
    "        with torch.no_grad():\n",
    "          return self.ViTnet.get_att(data)\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'new_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            index_to_remove = random.randint(0, len(self.memory) - 1)\n",
    "            self.memory.pop(index_to_remove)\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, n_patches):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(n_patches, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_patches)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc_layers(input)\n",
    "        return x\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, batch_size, att_dim, n_patches, buffer_size, gamma, tau, update_every, lr, env, device):\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # soft update parameter\n",
    "        self.tau = tau\n",
    "        # soft_update frequency\n",
    "        self.step = 0\n",
    "        self.update_every = update_every\n",
    "\n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "\n",
    "        # device (CPU o GPU)\n",
    "        self.device = device\n",
    "\n",
    "        # environment\n",
    "        self.env = env\n",
    "\n",
    "        # agent net\n",
    "        self.q_network = QNetwork(n_patches).to(self.device)\n",
    "        # target net\n",
    "        self.target_network = QNetwork(n_patches).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=lr, amsgrad=True)\n",
    "        # replay memory\n",
    "        self.memory = ReplayBuffer(buffer_size, batch_size)\n",
    "\n",
    "\n",
    "    def select_action(self, data, eps = 0.):\n",
    "\n",
    "        sample = random.random()\n",
    "\n",
    "        if sample > eps:\n",
    "            with torch.no_grad():\n",
    "                selected_batch = self.q_network(data)\n",
    "                selected = torch.mean(selected_batch, dim=0)\n",
    "                return selected\n",
    "        else:\n",
    "            selected = self.env.action_space.sample()\n",
    "            return selected\n",
    "\n",
    "\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample()\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        new_state_batch = torch.cat(batch.new_state).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "\n",
    "        state_action_values = self.q_network(state_batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_network(new_state_batch)\n",
    "\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch.unsqueeze(1)\n",
    "\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.q_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.step += 1\n",
    "        if self.step == self.update_every:\n",
    "            self.soft_update()\n",
    "            self.step = 0\n",
    "\n",
    "\n",
    "    def soft_update(self):\n",
    "        target_network_state_dict = self.target_network.state_dict()\n",
    "        q_network_state_dict = self.q_network.state_dict()\n",
    "        for key in q_network_state_dict:\n",
    "            target_network_state_dict[key] = q_network_state_dict[key]*self.tau + target_network_state_dict[key]*(1-self.tau)\n",
    "        self.target_network.load_state_dict(target_network_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "\n",
    "class TrainingTestingAgent():\n",
    "\n",
    "    def __init__(self, buffer_batch_size, get_reward_every, batch_size, model, att_dim, n_patches, epochs, env, buffer_size, gamma, tau, update_every, lr, eps_end, eps_start, eps_decay, train_loader, validation_loader, device):\n",
    "\n",
    "      self.env = env\n",
    "\n",
    "      self.epochs = epochs\n",
    "      self.eps_start = eps_start\n",
    "      self.eps_end = eps_end\n",
    "      self.eps_decay = eps_decay\n",
    "      self.get_reward_every = get_reward_every\n",
    "\n",
    "      self.batch_size = batch_size\n",
    "\n",
    "      self.validation_acc = []\n",
    "      self.validation_loss = []\n",
    "      self.epoch_time_list = []\n",
    "      self.validation_precision = []\n",
    "      self.validation_recall = []\n",
    "      self.validation_f1 = []\n",
    "\n",
    "      self.ViTnet = model\n",
    "\n",
    "      self.n_patches = n_patches\n",
    "\n",
    "      # creazione agente\n",
    "      self.agent = DQNAgent(buffer_batch_size, att_dim, n_patches, buffer_size, gamma, tau, update_every, lr, self.env, device)\n",
    "\n",
    "      self.train_loader = train_loader\n",
    "      self.validation_loader = validation_loader\n",
    "\n",
    "      self.device = device\n",
    "\n",
    "\n",
    "    def train_test(self):\n",
    "\n",
    "        step_reward = []\n",
    "        selected_patch_list = []\n",
    "        epoch = 0\n",
    "        iteration = 0\n",
    "        eps = self.eps_start\n",
    "\n",
    "        save_dir = \"results/patch_logs_mnist\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        while epoch < self.epochs:\n",
    "\n",
    "            epoch += 1\n",
    "            print(f'Epoch: {epoch}/{self.epochs}')\n",
    "\n",
    "            start_time = time.time()\n",
    "            samples = len(self.train_loader.dataset)\n",
    "\n",
    "            for i, (data, target) in enumerate(self.train_loader):\n",
    "                i += 1\n",
    "                iteration += 1\n",
    "\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                state = self.env.get_state(data)\n",
    "\n",
    "                # soft action 확률값\n",
    "                patch_probs = self.agent.select_action(state, eps)\n",
    "\n",
    "                # threshold 적용해서 0/1 binary mask 생성\n",
    "                # patch_list = (torch.tensor(patch_probs) > 0.5).int().tolist()\n",
    "                # patch_list = (torch.tensor(patch_probs) > 0.5).float()\n",
    "                # patch_list = torch.tensor(patch_probs).float()\n",
    "                self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n",
    "                hard_mask = self.ViTnet.get_patches()\n",
    "                if isinstance(hard_mask, list):\n",
    "                    hard_mask = torch.tensor(hard_mask, dtype = torch.float32)\n",
    "                patch_list = hard_mask.clone().detach().cpu()\n",
    "\n",
    "                # 로그 저장용\n",
    "                patch_log = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iteration\": iteration,\n",
    "                    \"batch_idx\": i,\n",
    "                    \"patch_list\": patch_list.tolist()\n",
    "                }\n",
    "\n",
    "                # 저장 경로\n",
    "                patch_log_path = os.path.join(save_dir, f\"epoch{epoch:03d}_batch{i:04d}.json\")\n",
    "                with open(patch_log_path, \"w\") as f:\n",
    "                    json.dump(patch_log, f)\n",
    "\n",
    "                # print도 동일하게 (눈으로 확인)\n",
    "                # print(f\"Patch list: {patch_list}\")\n",
    "                # print(f\"Selected Patches: {sum(patch_list)}\")\n",
    "\n",
    "                # 나머지 학습 로직 그대로 유지\n",
    "                if i % self.get_reward_every != 0:\n",
    "                    self.env.step_train(patch_list, data, target)\n",
    "                else:\n",
    "                    new_state, reward = self.env.step_reward(patch_list, data, target)\n",
    "                    print(f'  Epsilon: {eps},   Reward: {reward}')\n",
    "\n",
    "                    step_reward.append(reward)\n",
    "                    reward = torch.full((self.batch_size,), reward, dtype=torch.float32)\n",
    "\n",
    "                    if epoch != 1:\n",
    "                        self.agent.memory.push(state, new_state, reward)\n",
    "                        self.agent.optimize_model()\n",
    "\n",
    "                eps = self.eps_end + (self.eps_start - self.eps_end) * math.exp(-1. * iteration / self.eps_decay)\n",
    "\n",
    "            print(f'\\nInizio Testing')\n",
    "            loss, acc, precision, recall, f1 = self.evaluate_epoch(self.validation_loader, models_save_path, dataset_name, self.device)\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            self.validation_acc.append(acc)\n",
    "            self.validation_loss.append(loss)\n",
    "            self.validation_precision.append(precision)\n",
    "            self.validation_recall.append(recall)\n",
    "            self.validation_f1.append(f1)\n",
    "            self.epoch_time_list.append(epoch_time)\n",
    "\n",
    "            print(f'Epoch time: {epoch_time}')\n",
    "            print(\"#\" * 40)\n",
    "            print('Episode End')\n",
    "            print(\"#\" * 40)\n",
    "\n",
    "        return step_reward, selected_patch_list\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_epoch(self, data_load, models_save_path, dataset_name, device):\n",
    "        patches = self.ViTnet.get_patches()\n",
    "        self.ViTnet.set_patches(torch.tensor([1 for i in range(len(patches))], dtype=torch.float))\n",
    "\n",
    "        self.ViTnet.eval()\n",
    "\n",
    "\n",
    "        elements = 0\n",
    "        # predizioni corrette\n",
    "        csamp = 0\n",
    "        tloss = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in data_load:\n",
    "\n",
    "                elements += len(data)\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                predictions = self.ViTnet(data)\n",
    "\n",
    "                output = functional.log_softmax(predictions, dim=1)\n",
    "                loss = functional.nll_loss(output, target, reduction='sum')\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "\n",
    "                predictions = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "\n",
    "                tloss += loss.item()\n",
    "                csamp += pred.eq(target).sum()\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_targets.extend(target.cpu())\n",
    "\n",
    "        loss_val = tloss / elements\n",
    "        acc_val = (100.0 * csamp / elements).cpu()\n",
    "\n",
    "\n",
    "        print('\\n\\nAverage test loss: ' + '{:.4f}'.format(loss_val) +\n",
    "              '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "              '{:5}'.format(elements) + ' (' +\n",
    "              '{:4.2f}'.format(acc_val) + '%)\\n')\n",
    "\n",
    "\n",
    "        precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "        return loss_val, acc_val, precision, recall, f1\n",
    "\n",
    "\n",
    "    def train_info(self):\n",
    "\n",
    "        return {\n",
    "                'train_loss': self.env.train_loss_list,\n",
    "                'train_time': self.env.train_time_list,\n",
    "                }\n",
    "\n",
    "    def validation_info(self):\n",
    "        return {\n",
    "                'validation_loss': self.validation_loss,\n",
    "                'validation_acc': [tensor.item() for tensor in self.validation_acc],\n",
    "                'validation_precision': self.validation_precision,\n",
    "                'validation_recall': self.validation_recall,\n",
    "                'validation_f1': self.validation_f1,\n",
    "                'epoch_time': self.epoch_time_list\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_batch_size = 8\n",
    "buffer_size = 64\n",
    "\n",
    "gamma = 0.95\n",
    "\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps = eps_start\n",
    "eps_decay = 20000\n",
    "\n",
    "lr = 0.001 # 0.01\n",
    "\n",
    "tau = 0.1\n",
    "update_every = 2\n",
    "\n",
    "get_reward_every = 50\n",
    "\n",
    "n_patch_selected = 30\n",
    "\n",
    "time_weight = 20\n",
    "loss_weight = 1\n",
    "\n",
    "patch = 7\n",
    "patch_size = int(img_size/patch)\n",
    "\n",
    "att_dim = 128\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "ViTnet = SimpleAgentViT(\n",
    "    image_size = img_size,\n",
    "    patch_size = patch_size,\n",
    "    num_classes = len(classes),\n",
    "    dim = att_dim,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    channels = 1\n",
    ")\n",
    "\n",
    "# Sposta il modello sulla GPU (se disponibile)\n",
    "ViTnet.to(device)\n",
    "\n",
    "# definiamo l'ottimizzatore\n",
    "optimizer = optim.Adam(ViTnet.parameters(), lr=learning_rate)\n",
    "\n",
    "total_patches = patch**2\n",
    "\n",
    "env = ViTEnv(ViTnet, total_patches, optimizer, loss_weight, time_weight, device, n_patch_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrainingTestingAgent(epochs = epochs,\n",
    "                             model = ViTnet,\n",
    "                             get_reward_every = get_reward_every,\n",
    "                             buffer_batch_size = buffer_batch_size,\n",
    "                             batch_size = batch_size,\n",
    "                             env = env,\n",
    "                             att_dim = att_dim,\n",
    "                             n_patches = total_patches,\n",
    "                             buffer_size = buffer_size,\n",
    "                             gamma = gamma,\n",
    "                             tau = tau,\n",
    "                             update_every = update_every,\n",
    "                             lr = lr,\n",
    "                             eps_end = eps_end,\n",
    "                             eps_start = eps_start,\n",
    "                             eps_decay = eps_decay,\n",
    "                             train_loader = train_loader,\n",
    "                             validation_loader = validation_loader,\n",
    "                             device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 1.0\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.9975774688124749,   Reward: -8.473684210526315\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 1.135811371263297\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.995111608749824,   Reward: -7.28524126031565\n",
      "  Patch list: [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 1.1486811376877504\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9926519056379346,   Reward: -5.16710833599646\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 1.0554648414418564\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.9901983441036541,   Reward: -8.418219369084458\n",
      "  Patch list: [1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 1.416025741776798\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.987750908812215,   Reward: -5.952395310854781\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 1.4568891667811288\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9853095844671388,   Reward: -2.7536371490083447\n",
      "  Patch list: [0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 1.3408236360064683\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9828743558101403,   Reward: -4.974965837677742\n",
      "  Patch list: [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 1.937369053823564\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.9804452076210326,   Reward: -1.2205256830185414\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 2.050186394444726\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.9780221247176314,   Reward: -0.05507676345001089\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.5546  Accuracy: 7956/ 9500 (83.75%)\n",
      "\n",
      "Epoch time: 19.733385801315308\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 2.1093241751092493\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9746882027096526,   Reward: -4.206465298574962\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 2.438287157915283\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9722794943428724,   Reward: -1.7722391578741905\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 3.4996284606246193\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9698768002260643,   Reward: -0.7108978551648542\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 1.829859763131992\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.9674801053423822,   Reward: -3.4332981316048494\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 2.659123718237593\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.9650893947124752,   Reward: -0.49877101860451223\n",
      "  Patch list: [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 2.8053523633817603\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.9627046533943944,   Reward: -6.668331847144555\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 2.489932832261829\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.9603258664834984,   Reward: -4.87848822036975\n",
      "  Patch list: [0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 3.574733022872293\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9579530191123613,   Reward: -0.6357932929171803\n",
      "  Patch list: [0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 2.9621464645707505\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.9555860964506797,   Reward: -4.406274588060828\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.2970  Accuracy: 8658/ 9500 (91.14%)\n",
      "\n",
      "Epoch time: 19.907054662704468\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 3.6343835077084004\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9523294453712271,   Reward: -0.5761428080810731\n",
      "  Patch list: [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 3.9991381379175945\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9499765640848659,   Reward: -2.316651335766616\n",
      "  Patch list: [0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 4.043749706174923\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.9476295576550898,   Reward: 0.8858549693328177\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 3.6083371083090894\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9452884114131015,   Reward: -2.7074523653751212\n",
      "  Patch list: [1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 4.680195030414117\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.9429531107267292,   Reward: -0.5829628643227247\n",
      "  Patch list: [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 2.5713931525700264\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9406236410003356,   Reward: -3.744396321114184\n",
      "  Patch list: [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 1.8906107123493854\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.9382999876747277,   Reward: -8.635705077124298\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 5.205889628261592\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.9359821362270647,   Reward: 4.153258049314224\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 3.3719603296352556\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.9336700721707673,   Reward: 0.2140655927931503\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.2188  Accuracy: 8878/ 9500 (93.45%)\n",
      "\n",
      "Epoch time: 18.41785955429077\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 5.158390095354825\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.9304889010972034,   Reward: 3.0531269374600885\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 3.0169895898626455\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.9281905529766673,   Reward: -4.351431462768933\n",
      "  Patch list: [1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 2.2056914129341014\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.9258979435500762,   Reward: -8.320624376539582\n",
      "  Patch list: [1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 3.2274982053186414\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9236110584886137,   Reward: -0.983028110470832\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 3.3294373533603383\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.9213298834992406,   Reward: -1.9337205413765033\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 2.1012335297487397\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9190544043246062,   Reward: -4.214555943935471\n",
      "  Patch list: [0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.053771281376495\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.9167846067429578,   Reward: 0.8432449655870213\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 3.8849092316754765\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.9145204765680534,   Reward: -3.4835118209561022\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 5.553852599550126\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.912261999649072,   Reward: 2.3959578627080207\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1835  Accuracy: 8953/ 9500 (94.24%)\n",
      "\n",
      "Epoch time: 17.821871757507324\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 4.735912107231347\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.9091545591673056,   Reward: -3.6851405243475996\n",
      "  Patch list: [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.341447170297042\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.9069094802872991,   Reward: 0.07828927556020027\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 4.244741891174172\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.9046700070944641,   Reward: -3.1236791614574067\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 4.572954206353762\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.9024361255920857,   Reward: 1.4150594695116565\n",
      "  Patch list: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.012656217756994\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.9002078218183974,   Reward: 0.6968667440727838\n",
      "  Patch list: [0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.739518007094282\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8979850818464933,   Reward: 1.5289916913048085\n",
      "  Patch list: [0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 5.653865913387503\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.8957678917842412,   Reward: -0.6619235602967075\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 4.393024165672476\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8935562377741961,   Reward: 0.18249784988300277\n",
      "  Patch list: [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 18\n",
      "  loss_reward: 6.041208936162035\n",
      "  patches_reward: -0.631578947368421\n",
      "  Epsilon: 0.8913501059935134,   Reward: -6.590370011206386\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1863  Accuracy: 8946/ 9500 (94.17%)\n",
      "\n",
      "Epoch time: 17.61218523979187\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.1093056826814935\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8883146872359479,   Reward: 0.8987793668920201\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 2.285962859620663\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.8861216429654066,   Reward: -5.082458193010916\n",
      "  Patch list: [0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 4.368540309293732\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.8839340744579859,   Reward: 1.210645572451627\n",
      "  Patch list: [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 5.905486268317947\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.8817519680413753,   Reward: -5.673461100103106\n",
      "  Patch list: [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 4.156513839999091\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8795753100774027,   Reward: -4.264538791579856\n",
      "  Patch list: [0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 4.822812440461263\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8774040869619488,   Reward: 0.6122861246717894\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.1333265457489246\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.875238285124862,   Reward: -0.18246292793528607\n",
      "  Patch list: [0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 2.68629931049731\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.8730778910298738,   Reward: -6.787384900029005\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.526926871338492\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.8709228911745139,   Reward: 0.21113739765428097\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1545  Accuracy: 9038/ 9500 (95.14%)\n",
      "\n",
      "Epoch time: 15.642863035202026\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 7.806637762331227\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8679578248802939,   Reward: -0.61441486924772\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 11.205450071192324\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.8658156092034264,   Reward: 0.6791342817186408\n",
      "  Patch list: [1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 4.166535988444888\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8636787423769023,   Reward: -1.0966219062919533\n",
      "  Patch list: [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 4.793167439822773\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.8615472110452971,   Reward: -5.73314834965091\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 3.608072042637241\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8594210018865327,   Reward: -4.812980588941706\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 4.803687675880508\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8573001016117953,   Reward: -0.45947021885633355\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 3.818325331863342\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.8551844969654511,   Reward: -2.4974641418208687\n",
      "  Patch list: [1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 2.000910353553035\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8530741747249642,   Reward: -6.420142278025912\n",
      "  Patch list: [1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 3.961456538475639\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8509691217008135,   Reward: -4.459596093103308\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1452  Accuracy: 9076/ 9500 (95.54%)\n",
      "\n",
      "Epoch time: 17.53183937072754\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 3.3576311591057615\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8480727772978519,   Reward: -1.90552673563108\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 5.24737850192891\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8459802121509186,   Reward: -3.1736741296500366\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 4.0400276484644495\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.8438928718830323,   Reward: -3.3283934041671293\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 7.191811290631878\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.84181074344831,   Reward: -2.281872919894437\n",
      "  Patch list: [1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.964558290456078\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8397338138334418,   Reward: 0.7014003957192365\n",
      "  Patch list: [1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.2131303519739465\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8376620700576112,   Reward: -0.050027542762895116\n",
      "  Patch list: [1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 1.845462094995741\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.8355954991724125,   Reward: -5.522958957635838\n",
      "  Patch list: [1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 7.10089213637326\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.8335340882617711,   Reward: -3.425423653100423\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.882767071846718\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.8314778244418622,   Reward: -1.4330224018374924\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1242  Accuracy: 9130/ 9500 (96.11%)\n",
      "\n",
      "Epoch time: 17.79284358024597\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 7.712596759797932\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8286486091501434,   Reward: 3.5020704440084582\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.3366361151630635\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.8266045437736059,   Reward: 1.12610979937359\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 4.503793361859336\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.8245655821781254,   Reward: -2.8646276907722426\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 6.298479193765589\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.8225317116201849,   Reward: -3.175205016760726\n",
      "  Patch list: [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 7.775545754019729\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8205029193880872,   Reward: -0.6455068775592183\n",
      "  Patch list: [0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 7.1966070468961965\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8184791928018738,   Reward: 1.9334491521593549\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 5.479011303409783\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.8164605192132474,   Reward: -2.9420413281691635\n",
      "  Patch list: [1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 6.004270938206838\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.814446886005491,   Reward: -1.3641501144247403\n",
      "  Patch list: [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.278646832961158\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.812438280593391,   Reward: -0.03714264072305262\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1063  Accuracy: 9172/ 9500 (96.55%)\n",
      "\n",
      "Epoch time: 18.06332278251648\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 11.024506624228414\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.8096746385490572,   Reward: 9.971875045281045\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.658349413646754\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.8076779488547449,   Reward: 0.395191518909912\n",
      "  Patch list: [0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.13896598273025\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.8056862446502095,   Reward: -0.17682349095396077\n",
      "  Patch list: [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 9.216163381731471\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.8036995134872933,   Reward: 7.110900223836735\n",
      "  Patch list: [1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 5.709171947757754\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.8017177429489201,   Reward: 2.5512772109156483\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 12.882148596438515\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7997409206490175,   Reward: 1.3032012280174623\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 6.997681108445746\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7977690342324398,   Reward: -4.581266259975307\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 4.08730135089195\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.7958020713748902,   Reward: -5.386382859634365\n",
      "  Patch list: [1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 3.996251779803621\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7938400197828449,   Reward: -1.2669061149332208\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1073  Accuracy: 9178/ 9500 (96.61%)\n",
      "\n",
      "Epoch time: 17.686234712600708\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 14.282082265199747\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.7911404311825835,   Reward: 13.22945068625238\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 7.540092271913762\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.7891900191355253,   Reward: 5.434829114019026\n",
      "  Patch list: [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 5.849918954442093\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.7872444770286232,   Reward: 2.692024217599988\n",
      "  Patch list: [1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.565247530636475\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7853037927022327,   Reward: 1.302089635899633\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 6.683810983653931\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.7833679540270703,   Reward: -1.737241647925016\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.9131118811759285\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.781436948904138,   Reward: 1.702585565386455\n",
      "  Patch list: [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 6.207450474815784\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.7795107652646475,   Reward: 3.0495557379736784\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 9.522596978670004\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7775893910699448,   Reward: -2.0563503897510493\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 16.976122966217687\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7756728143114351,   Reward: 5.397175597796634\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0868  Accuracy: 9231/ 9500 (97.17%)\n",
      "\n",
      "Epoch time: 17.64978337287903\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.928886796439706\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7730357945766965,   Reward: 1.665728901702864\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 13.203977751353118\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7711305875912817,   Reward: 1.6250303829320654\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 8.42262735407276\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.769230137674517,   Reward: 1.054206301441182\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.312143063030106\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7673344329485843,   Reward: 1.0489851682932647\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 4.931456882286255\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.7654434615653228,   Reward: 0.7209305664967811\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 5.17841224998918\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.7635572117061551,   Reward: -3.2426403815897666\n",
      "  Patch list: [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 4.784380324865666\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.7616756715820137,   Reward: 1.6264855880235602\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.9964508539654\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7597988294332664,   Reward: -1.3193386197188106\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 13.88986364661599\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.7579266735296438,   Reward: 2.310916278194936\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.1008  Accuracy: 9194/ 9500 (96.78%)\n",
      "\n",
      "Epoch time: 17.99319553375244\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 3.5281769979587914\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7553507724902309,   Reward: -2.7876124757254193\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 8.140885916658524\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.7534897228403642,   Reward: 3.9303596008690507\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.389122016651426\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7516333200036855,   Reward: 1.0733325429672158\n",
      "  Patch list: [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 6.587227288218608\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.7497815523776711,   Reward: 2.376700972429134\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 16.99692092317251\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.747934408388767,   Reward: 7.523236712646195\n",
      "  Patch list: [0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 6.3547619371421575\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.7460918764923177,   Reward: -2.0662906944367894\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 9.954801659703254\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.7442539451724924,   Reward: 0.48111744917693855\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.812216829746137\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7424206029422147,   Reward: 0.49642735606192634\n",
      "  Patch list: [0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.065932829747675\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7405918383430895,   Reward: 0.7501433560634645\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0855  Accuracy: 9245/ 9500 (97.32%)\n",
      "\n",
      "Epoch time: 17.38324499130249\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 5.868583456351702\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.7380756394396685,   Reward: -1.4998375962798765\n",
      "  Patch list: [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 9.693517172513157\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.7362577236825968,   Reward: -0.8327986169605257\n",
      "  Patch list: [0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.07300568873508\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.734444347038662,   Reward: -2.2427837849491308\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.939942495446858\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7326354981742547,   Reward: 1.6241530217626474\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 8.22634180135711\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.730831165784063,   Reward: -1.247342409169205\n",
      "  Patch list: [0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 13.531775418676641\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7290313385910041,   Reward: 8.268617523939799\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 5.53746204599321\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.727236005346152,   Reward: -2.883590585585737\n",
      "  Patch list: [1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 6.524045844374781\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.725445154828668,   Reward: 2.3135195285853074\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.679088877250884\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.7236587758457306,   Reward: 5.521194140408778\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0850  Accuracy: 9231/ 9500 (97.17%)\n",
      "\n",
      "Epoch time: 17.481978178024292\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 12.960721041921468\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.7212008953508263,   Reward: 9.802826305079364\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.58823664555319\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7194251137643185,   Reward: 3.325078750816348\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 18\n",
      "  loss_reward: 7.466875122817095\n",
      "  patches_reward: -0.631578947368421\n",
      "  Epsilon: 0.7176537660870811,   Reward: -5.164703824551326\n",
      "  Patch list: [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 5.093327313709901\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7158868412481852,   Reward: -1.2224621599743095\n",
      "  Patch list: [1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 8.536014185164886\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.714124328204345,   Reward: 4.325487869375412\n",
      "  Patch list: [1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 12.504782624065289\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7123662159398482,   Reward: 7.241624729328447\n",
      "  Patch list: [1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.109364786898749\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.7106124934664874,   Reward: 0.8462068921619075\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.8131277161871475\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7088631498234912,   Reward: -1.502661757497063\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 18\n",
      "  loss_reward: 17.358923904320193\n",
      "  patches_reward: -0.631578947368421\n",
      "  Epsilon: 0.7071181740774565,   Reward: 4.727344956951772\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0753  Accuracy: 9260/ 9500 (97.47%)\n",
      "\n",
      "Epoch time: 17.749845504760742\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 5.743988149932852\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.7047172603345017,   Reward: -0.5718013237513588\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 11.55739305845381\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.7029826363670746,   Reward: 2.083708847927495\n",
      "  Patch list: [1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 8.366747612516829\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.7012523435433805,   Reward: 4.156221296727355\n",
      "  Patch list: [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 8.280584773124719\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6995263710490839,   Reward: 1.9647952994405085\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 4.5798939057424555\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6978047080968508,   Reward: 0.369367589952982\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 16.266378484973895\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.6960873439262824,   Reward: 6.79269427444758\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 11.036662342835548\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6943742678038468,   Reward: 2.6156097112566012\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 11.898096132737365\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6926654690228128,   Reward: 3.477043501158418\n",
      "  Patch list: [0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 8.13668679613437\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.6909609369031823,   Reward: 6.031423638239634\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0750  Accuracy: 9272/ 9500 (97.60%)\n",
      "\n",
      "Epoch time: 17.277143001556396\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 11.237801486879327\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6886156695832049,   Reward: 2.8167488553003803\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 5.601032019292696\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.68692124931709,   Reward: -3.8726521912336187\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 14.769279133238134\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.6852310598109868,   Reward: 9.506121238501294\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 15.49759786553627\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6835450905012054,   Reward: 8.12917681290469\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 7.8918164644802244\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.6818633308504323,   Reward: -2.6344993249934587\n",
      "  Patch list: [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.2152697005871\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.680185770347664,   Reward: 23.110006542692364\n",
      "  Patch list: [1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 4.193476220411001\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.678512398508142,   Reward: 1.0355814835688957\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 25.255386278417728\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6768432048732869,   Reward: 17.88696522578615\n",
      "  Patch list: [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 8.909658374820479\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.6751781790106328,   Reward: -0.5640258357058361\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0681  Accuracy: 9288/ 9500 (97.77%)\n",
      "\n",
      "Epoch time: 17.726144313812256\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.89085537076077\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.6728872683861699,   Reward: 3.6276974760239282\n",
      "  Patch list: [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 16.32975228955947\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6712321200127275,   Reward: 10.013962815875258\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 8.679595395498968\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.6695811043421878,   Reward: -2.8993519729220854\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 21.573086647724725\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.6679342110556973,   Reward: 11.046770858251042\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 9.172356395312827\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6662914298601675,   Reward: 2.856566921628616\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 7.613849059711398\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6646527504882108,   Reward: 4.455954322869292\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 15.711346314449314\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.6630181626980757,   Reward: 6.2376621039229985\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 15.805883370675117\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.6613876562735831,   Reward: 6.332199160148802\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 11.466277561447015\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.6597612210240628,   Reward: -0.11266980697403817\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0680  Accuracy: 9295/ 9500 (97.84%)\n",
      "\n",
      "Epoch time: 17.56582498550415\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 8.142528813584866\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6575234072599039,   Reward: -0.2785238179940812\n",
      "  Patch list: [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 19.777779922679166\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6559066205671964,   Reward: 15.567253606889693\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 18.254022994952884\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.65429387079297,   Reward: 8.780338784426569\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 35.28104999966839\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6526851478575332,   Reward: 28.965260525984178\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 13.680563892888996\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6510804417063627,   Reward: 5.259511261310049\n",
      "  Patch list: [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 7.105018391240588\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6494797423100398,   Reward: -1.3160342403383591\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 11.41769539766514\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6478830396641878,   Reward: 5.1019059239809295\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 14.019128551279978\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6462903237894103,   Reward: 10.861233814437874\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 10.372908085074151\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6447015847312276,   Reward: 4.0571186113899405\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0639  Accuracy: 9311/ 9500 (98.01%)\n",
      "\n",
      "Epoch time: 17.908336877822876\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 9.215390909103023\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.6425156371915971,   Reward: 3.9522330143661817\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.8762179946725315\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6409363230638371,   Reward: 0.5604285209883209\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 11.075056225047845\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6393609522901501,   Reward: 6.864529909258372\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 10.298580592319578\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.6377895150244637,   Reward: 5.035422697582736\n",
      "  Patch list: [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 8.164208161746016\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6362220014452897,   Reward: 0.7957871091144373\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 11.42532273410417\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.6346584017556633,   Reward: 5.10953326041996\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 9.218160252316144\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6330987061830813,   Reward: 1.8497391996845653\n",
      "  Patch list: [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.595375050088544\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6315429049794411,   Reward: 5.437480313246438\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 11.16361767931079\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6299909884209803,   Reward: 3.7951966266792105\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0664  Accuracy: 9294/ 9500 (97.83%)\n",
      "\n",
      "Epoch time: 17.426125049591064\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 9.991974778867187\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.6278557049927755,   Reward: 4.728816884130345\n",
      "  Patch list: [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 10.01211131476157\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.6263129949213776,   Reward: 1.5910586831826237\n",
      "  Patch list: [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 8.884071555589955\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.6247741368082042,   Reward: 7.831439976642587\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 13.769804822958031\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6232391210353869,   Reward: 10.611910086115927\n",
      "  Patch list: [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 10.777245907724732\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6217079380090724,   Reward: 7.619351170882626\n",
      "  Patch list: [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 5.808761457301948\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6201805781593618,   Reward: 2.6508667204598426\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 20.3306992539911\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6186570319402508,   Reward: 16.120172938201627\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.197012923497572\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6171372898295707,   Reward: 0.9864866077080983\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 15.23538713769385\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.6156213423289286,   Reward: 7.866966085062272\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0689  Accuracy: 9289/ 9500 (97.78%)\n",
      "\n",
      "Epoch time: 17.635667085647583\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 19.69832276762746\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6135355487606451,   Reward: 15.487796451837987\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 30\n",
      "  loss_reward: 22.83788688904375\n",
      "  patches_reward: 0.0\n",
      "  Epsilon: 0.6120285943666081,   Reward: 22.83788688904375\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 9.723042911998112\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6105254026532455,   Reward: 6.565148175156007\n",
      "  Patch list: [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 6.980948428732176\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.6090259642256044,   Reward: -2.4927357817941393\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 12.569609343164187\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6075302697121896,   Reward: 9.411714606322082\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.57406656977546\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.6060383097649056,   Reward: 5.4161718329333555\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 8.6496336620543\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6045500750589979,   Reward: 4.439107346264827\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 9.94559339332939\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6030655562929947,   Reward: 5.735067077539916\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 25.589189499205844\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.6015847441886488,   Reward: 21.37866318341637\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0547  Accuracy: 9347/ 9500 (98.39%)\n",
      "\n",
      "Epoch time: 17.478638410568237\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.832940272181942\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5995472934446275,   Reward: 5.675045535339836\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 6.310253338999121\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5980752660119876,   Reward: 3.1523586021570154\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 6.366315690245956\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5966069140516747,   Reward: 2.155789374456482\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 9.73497653505742\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5951422283864839,   Reward: 6.577081798215315\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 16.910449602432536\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5936811998621254,   Reward: 11.647291707695693\n",
      "  Patch list: [0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 15.576177114055165\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5922238193471661,   Reward: 10.313019219318324\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 14.732125067516131\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.590770077732973,   Reward: 10.521598751726657\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 21.646139698402493\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5893199659336562,   Reward: 17.43561338261302\n",
      "  Patch list: [0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 7.392947562926572\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5878734748860123,   Reward: 2.129789668189731\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0602  Accuracy: 9318/ 9500 (98.08%)\n",
      "\n",
      "Epoch time: 17.659530878067017\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 7.842857640231275\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5858832465156517,   Reward: 0.47443658759969587\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.000561781829434\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5844453365357488,   Reward: 2.7374038870925927\n",
      "  Patch list: [0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 9.787073768330705\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5830110168410694,   Reward: 5.576547452541232\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 14.65091417459471\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5815802784671102,   Reward: 10.440387858805238\n",
      "  Patch list: [0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.761279887561711\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5801531124717522,   Reward: 3.4981219928248697\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 5.246318824584962\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.5787295099352031,   Reward: -4.227365385941353\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.626566097780609\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5773094619599425,   Reward: 5.468671360938504\n",
      "  Patch list: [1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 11.35847300746635\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5758929596706656,   Reward: 8.200578270624245\n",
      "  Patch list: [1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 8.718481616163196\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.574479994214229,   Reward: 0.2974289845842488\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0587  Accuracy: 9324/ 9500 (98.15%)\n",
      "\n",
      "Epoch time: 18.30204200744629\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 12.198136614355828\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5725358937358194,   Reward: 5.882347140671618\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.112101327460774\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5711313104621256,   Reward: 2.8489434327239325\n",
      "  Patch list: [0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 16.690812092636367\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5697302342609489,   Reward: 12.480285776846895\n",
      "  Patch list: [1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 20.053724330600758\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5683326563755584,   Reward: 16.895829593758652\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.115413256027352\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5669385680710877,   Reward: 11.010150098132616\n",
      "  Patch list: [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.730111601684637\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5655479606344803,   Reward: 3.3616905490530584\n",
      "  Patch list: [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.725215705607699\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5641608253744353,   Reward: 3.4620578108708573\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 12.030661756142019\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5627771536213529,   Reward: 8.872767019299914\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.651634216499383\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5613969367272801,   Reward: 0.38847632176254177\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0596  Accuracy: 9317/ 9500 (98.07%)\n",
      "\n",
      "Epoch time: 17.938764095306396\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 7.276401214454602\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.559497895026117,   Reward: -0.09201983817697634\n",
      "  Patch list: [1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 19.668779838238564\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5581258660393836,   Reward: 14.405621943501721\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 8.678634695034136\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5567572628410971,   Reward: 5.5207399581920304\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 8.934913909911527\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5553920768774832,   Reward: 6.829650752016791\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 15.249261751122809\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5540302996161249,   Reward: 13.143998593228073\n",
      "  Patch list: [0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 25.50195861478443\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5526719225459105,   Reward: 20.23880072004759\n",
      "  Patch list: [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 13.61167715235226\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5513169371769783,   Reward: 7.29588767866805\n",
      "  Patch list: [1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 9.08203818746796\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5499653350406657,   Reward: 1.7136171348363813\n",
      "  Patch list: [1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.324879339718551\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5486171076894546,   Reward: 0.009089866034340588\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0555  Accuracy: 9329/ 9500 (98.20%)\n",
      "\n",
      "Epoch time: 18.124783277511597\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 20.156034397394553\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5467620804299036,   Reward: 19.103402818447183\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 17.814787280027243\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.5454218512133854,   Reward: 9.393734648448296\n",
      "  Patch list: [0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.8290170086125395\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5440849683851803,   Reward: -1.4867724650716712\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 13.191253778670976\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5427514235897661,   Reward: 5.8228327260393975\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 30\n",
      "  loss_reward: 10.48935442005905\n",
      "  patches_reward: 0.0\n",
      "  Epsilon: 0.5414212084924837,   Reward: 10.48935442005905\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 30\n",
      "  loss_reward: 17.505433344314902\n",
      "  patches_reward: 0.0\n",
      "  Epsilon: 0.540094314779484,   Reward: 17.505433344314902\n",
      "  Patch list: [1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 9.186067318152078\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.5387707341576774,   Reward: 6.028172581309973\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 11.457370054900109\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5374504583546805,   Reward: 10.40473847595274\n",
      "  Patch list: [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 5.5829398853707755\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5361334791187654,   Reward: 1.372413569581302\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0564  Accuracy: 9326/ 9500 (98.17%)\n",
      "\n",
      "Epoch time: 18.483426094055176\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 6.7306557381995225\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.534321446169952,   Reward: -1.6903968933794244\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 17.264142131823828\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.533012279694479,   Reward: 16.211510552876458\n",
      "  Patch list: [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 24.241683317806096\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5317063820474566,   Reward: 20.031157002016624\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.676150737804795\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5304037450670201,   Reward: 0.3603612641205842\n",
      "  Patch list: [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 14.111001810901115\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.5291043606116844,   Reward: 5.689949179322168\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 11.03993148358707\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5278082205602924,   Reward: 6.829405167797597\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 22.86187075831238\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5265153168119644,   Reward: 21.80923917936501\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 18.515069407827056\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5252256412860478,   Reward: 14.304543092037584\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 20.937205947233174\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5239391859220665,   Reward: 19.884574368285804\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0504  Accuracy: 9345/ 9500 (98.37%)\n",
      "\n",
      "Epoch time: 18.415295839309692\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 5.800846234609961\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5221691507968794,   Reward: -1.567574818021618\n",
      "  Patch list: [0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.50742966850425\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5208903271155428,   Reward: 3.1390086158726707\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 12.569392308954795\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5196146965004137,   Reward: 11.516760730007427\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 39.93098367633077\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.5183422509787968,   Reward: 38.8783520973834\n",
      "  Patch list: [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 13.474164287712751\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.517072982597903,   Reward: 8.21100639297591\n",
      "  Patch list: [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 14.219928779303332\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5158068834248013,   Reward: 6.851507726671754\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 14.7943089313496\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5145439455463674,   Reward: 10.583782615560125\n",
      "  Patch list: [0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 11.71831184478598\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.5132841610692356,   Reward: 7.507785528996506\n",
      "  Patch list: [1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.696667628222727\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5120275221197488,   Reward: 3.328246575591148\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0497  Accuracy: 9349/ 9500 (98.41%)\n",
      "\n",
      "Epoch time: 17.945298433303833\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 12.997716724837856\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.510298511426843,   Reward: 10.89245356694312\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.346321237779504\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.5090493272790773,   Reward: 1.0831633430426626\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.695411887651144\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5078032621912315,   Reward: 11.590148729756407\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 20.147059061303352\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.506560308375395,   Reward: 18.041795903408616\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 12.38272276723357\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.5053204580631021,   Reward: 5.0143017146019915\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 23.928196300720636\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5040837035052846,   Reward: 21.8229331428259\n",
      "  Patch list: [0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 11.135617702371945\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.5028500369722223,   Reward: 4.819828228687735\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 26.118551255899746\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5016194507534955,   Reward: 24.01328809800501\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 31.901217118540472\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.5003919371579361,   Reward: 29.795953960645736\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0548  Accuracy: 9334/ 9500 (98.25%)\n",
      "\n",
      "Epoch time: 18.180184602737427\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 23.204543048586387\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.4987030000664382,   Reward: 22.151911469639018\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.270716859901826\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.4974827684912782,   Reward: -0.04507261378238425\n",
      "  Patch list: [0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 23.101335455412986\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4962655836850083,   Reward: 17.838177560676144\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 29\n",
      "  loss_reward: 31.235380093187285\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.49505143804021917,   Reward: 30.182748514239915\n",
      "  Patch list: [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 8.542327324723347\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.49384032396849675,   Reward: 4.331801008933874\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 7.0166363958916165\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4926322339003742,   Reward: 1.7534785011547749\n",
      "  Patch list: [0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 6.75056286206563\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.49142716028528455,   Reward: -0.6178581905659488\n",
      "  Patch list: [1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 29.32018036957256\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.49022509559151384,   Reward: 26.162285632730455\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.525968286390402\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4890260323061539,   Reward: 3.2628103916535602\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0446  Accuracy: 9360/ 9500 (98.53%)\n",
      "\n",
      "Epoch time: 18.509364366531372\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.027198539369126\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.4873762400227738,   Reward: 0.711409065684915\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 10.798276616671187\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.48618428998107627,   Reward: 8.69301345877645\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 33.10308752305997\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.48499531609274116,   Reward: 30.997824365165233\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "  Selected Patches: 31\n",
      "  loss_reward: 12.199465353653705\n",
      "  patches_reward: -0.05263157894736842\n",
      "  Epsilon: 0.48380931092667784,   Reward: 11.146833774706337\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 41.41702338704757\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4826262670703502,   Reward: 39.31176022915283\n",
      "  Patch list: [1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 5.874176415586675\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4814461771297302,   Reward: 0.6110185208498331\n",
      "  Patch list: [1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.5513155040571815\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.48026903372925195,   Reward: -1.764473969627029\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 18.94090247221465\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4790948295117653,   Reward: 16.835639314319913\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 17.563126713099138\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4779235571384901,   Reward: 15.457863555204401\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0451  Accuracy: 9358/ 9500 (98.51%)\n",
      "\n",
      "Epoch time: 15.613544464111328\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 21.162579162192213\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4763120023967523,   Reward: 19.057316004297476\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 35.472796262741674\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4751476784021724,   Reward: 33.367533104846935\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.410669825657788\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.47398626158209656,   Reward: 23.30540666776305\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 5.836056171959304\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.47282774467766603,   Reward: -3.6376280385670112\n",
      "  Patch list: [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 19\n",
      "  loss_reward: 4.8355269653258\n",
      "  patches_reward: -0.5789473684210527\n",
      "  Epsilon: 0.4716721204481463,   Reward: -6.743420403095253\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 15.661428507856785\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.47051938167088225,   Reward: 13.556165349962049\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 11.019521192135818\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4693695211412527,   Reward: 8.914258034241081\n",
      "  Patch list: [0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 16.310343489760495\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.4682225316726257,   Reward: 12.099817173971022\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 18.257237306840427\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4670784060963131,   Reward: 16.15197414894569\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0396  Accuracy: 9376/ 9500 (98.69%)\n",
      "\n",
      "Epoch time: 17.720405340194702\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 11.736227747414084\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.4655042026576253,   Reward: 8.578333010571978\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.748452556531404\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4643668644161467,   Reward: 23.643189398636668\n",
      "  Patch list: [1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 7.68422481907871\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.46323236596904976,   Reward: 0.3158037664471314\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 9.28955766161008\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.4621007002257155,   Reward: 1.921136608978502\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 19.249133484631102\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4609718601132293,   Reward: 17.143870326736366\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 9.016173604374671\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.4598458385763369,   Reward: -1.5101421850990118\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.964435608169355\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4587226285773998,   Reward: 11.859172450274619\n",
      "  Patch list: [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 14.028406423881227\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.4576022230963521,   Reward: 10.870511687039123\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 21.63715935898479\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.45648461513065575,   Reward: 19.531896201090053\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0399  Accuracy: 9383/ 9500 (98.77%)\n",
      "\n",
      "Epoch time: 19.79680347442627\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 69.39981900227718\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.45494689729694177,   Reward: 67.29455584438244\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 12.188086311338397\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.45383591935476136,   Reward: 10.08282315344366\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 20.232056937190485\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4527277153885218,   Reward: 18.12679377929575\n",
      "  Patch list: [1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 8.214764284319404\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.4516222784719445,   Reward: 1.8989748106351936\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 28.21100904893851\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4505196016960453,   Reward: 26.105745891043775\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 18.301326375488866\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4494196781690907,   Reward: 16.19606321759413\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 16.234832808673662\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.44832250101655496,   Reward: 14.129569650778926\n",
      "  Patch list: [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 14.05318185854654\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.4472280633810775,   Reward: 6.68476080591496\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 6.379929698889024\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4461363584224194,   Reward: 1.1167718041521821\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0409  Accuracy: 9375/ 9500 (98.68%)\n",
      "\n",
      "Epoch time: 18.02355694770813\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 15.847509192843532\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.4446342805600478,   Reward: 11.63698287705406\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 12.082669079604045\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.4435490519596214,   Reward: 7.872142763814572\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 21.983227821795886\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.44246653304218103,   Reward: 19.87796466390115\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 12.518678976160636\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4413867170419799,   Reward: 10.4134158182659\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 19.418188240549792\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.44030959721016455,   Reward: 17.312925082655056\n",
      "  Patch list: [0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 15.80358796974715\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.43923516681473257,   Reward: 13.698324811852414\n",
      "  Patch list: [0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 25.40110314926821\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.43816341914049045,   Reward: 20.137945254531367\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 15.101996624359424\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4370943474890116,   Reward: 12.996733466464688\n",
      "  Patch list: [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 6.856851412433188\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.43602794517859494,   Reward: -3.6694643770404953\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0392  Accuracy: 9377/ 9500 (98.71%)\n",
      "\n",
      "Epoch time: 18.19527506828308\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 23.910907340304327\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4345606812533419,   Reward: 21.80564418240959\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 16.241581474576357\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4335006051974014,   Reward: 14.13631831668162\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 45.676060371816725\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4324431760216219,   Reward: 43.570797213921985\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 29.776612081149537\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4313883871170678,   Reward: 27.6713489232548\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 26.221519758867256\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4303362318913047,   Reward: 24.11625660097252\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 12.30185067847063\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.4292867037683593,   Reward: 7.038692783733789\n",
      "  Patch list: [1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.922656249687182\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.42823979618867736,   Reward: 0.6068667760029713\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 34.01601128747379\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.427195502609083,   Reward: 31.91074812957905\n",
      "  Patch list: [1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.080956092552151\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.426153816502738,   Reward: 2.7125350399205725\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0403  Accuracy: 9376/ 9500 (98.69%)\n",
      "\n",
      "Epoch time: 15.303540706634521\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 45.37616306908019\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4247205596255281,   Reward: 43.27089991118545\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 17.03217934850973\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.4236850531488863,   Reward: 13.874284611667623\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 18.35255072734146\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4226521322051734,   Reward: 16.247287569446723\n",
      "  Patch list: [1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 15.873382493139658\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4216217903386299,   Reward: 13.768119335244922\n",
      "  Patch list: [1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 7.8707936320056815\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.420594021109616,   Reward: 3.660267316216208\n",
      "  Patch list: [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 12.56376232020707\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.41956881809457064,   Reward: 6.24797284652286\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 26.556225801196344\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.41854617488597157,   Reward: 24.450962643301608\n",
      "  Patch list: [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 6.8874805992791055\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.4175260850922955,   Reward: 0.5716911255948949\n",
      "  Patch list: [1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 20.835480200439374\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.41650854233797774,   Reward: 16.6249538846499\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0406  Accuracy: 9376/ 9500 (98.69%)\n",
      "\n",
      "Epoch time: 17.773231744766235\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 20.454465387794077\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.4151085043211532,   Reward: 13.086044335162498\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 17.367494519782934\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4140969979701153,   Reward: 15.262231361888198\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 21.814547851591534\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.41308801722663013,   Reward: 19.709284693696798\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 27.98062532153061\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.41208155578456473,   Reward: 25.875362163635874\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 38.09231552235731\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4110776073535319,   Reward: 35.98705236446257\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 22.90505966410442\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4100761656588506,   Reward: 20.799796506209685\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 20\n",
      "  loss_reward: 5.250802089015177\n",
      "  patches_reward: -0.5263157894736842\n",
      "  Epsilon: 0.4090772244415069,   Reward: -5.275513700458506\n",
      "  Patch list: [1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 16.002639791760046\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.40808077745811516,   Reward: 9.686850318075836\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 22.295545453167954\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.40708681848087835,   Reward: 20.190282295273217\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0373  Accuracy: 9384/ 9500 (98.78%)\n",
      "\n",
      "Epoch time: 18.102063179016113\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 54.93243018766386\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.40571922940475275,   Reward: 52.82716702976912\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 50.59418537744494\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4047311669239577,   Reward: 48.4889222195502\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 46.59835005216572\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.4037455715142408,   Reward: 43.44045531532362\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.396033399281155\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.4027624370156277,   Reward: 23.29077024138642\n",
      "  Patch list: [1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 4.31833766220791\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.4017817572835244,   Reward: -1.9974518114763002\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 24.093970876804455\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.4008035261886794,   Reward: 20.93607613996235\n",
      "  Patch list: [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 13.82553434154977\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3998277376171453,   Reward: 10.667639604707663\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 45.70138909081297\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3988543854702402,   Reward: 42.543494353970864\n",
      "  Patch list: [1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.183105114575591\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.39788346366451016,   Reward: 2.9199472198387495\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0401  Accuracy: 9357/ 9500 (98.49%)\n",
      "\n",
      "Epoch time: 18.12838125228882\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.763881131027958\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.39654757145396863,   Reward: 11.658617973133222\n",
      "  Patch list: [0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 7.691336068302228\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.39558240948048906,   Reward: -0.7297165632767193\n",
      "  Patch list: [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 15.051585594965934\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.3946196573983239,   Reward: 6.630532963386987\n",
      "  Patch list: [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 97.81862245400868\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3936593091902695,   Reward: 95.71335929611394\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 16.317875069238514\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3927013588541463,   Reward: 13.159980332396408\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 17.36501641115534\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.39174580040276186,   Reward: 15.259753253260605\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 8.427349717023864\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.39079262786387253,   Reward: 3.1641918222870222\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 21.227135432373593\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.38984183528014693,   Reward: 19.121872274478857\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 23.943910188856737\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.38889341670912825,   Reward: 21.838647030962\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0384  Accuracy: 9379/ 9500 (98.73%)\n",
      "\n",
      "Epoch time: 19.604265928268433\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.813496336800167\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.38758848672004015,   Reward: 3.4450752841685883\n",
      "  Patch list: [1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 29.871432071694453\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3866456944845719,   Reward: 27.766168913799717\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 14.637795178092409\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.38570525628592034,   Reward: 12.532532020197673\n",
      "  Patch list: [0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 23.760088448934688\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.3847671662463435,   Reward: 19.549562133145216\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.916047242266023\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3838314185027757,   Reward: 11.810784084371287\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 26.546867633034942\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.38289800720679035,   Reward: 24.441604475140206\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 22.133493699966106\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.381966926524564,   Reward: 18.975598963124\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 14.65091107076502\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.3810381706368392,   Reward: 8.335121597080809\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 17.630792842265205\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.38011173373888874,   Reward: 14.472898105423099\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0405  Accuracy: 9376/ 9500 (98.69%)\n",
      "\n",
      "Epoch time: 17.538994789123535\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 23.289853387386277\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.37883704835410664,   Reward: 21.18459022949154\n",
      "  Patch list: [0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 7.114799429674126\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.3779161073890843,   Reward: -0.2536216229574526\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 33.13433457607789\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.37699746590093086,   Reward: 31.029071418183154\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 19.780960624146548\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.37608111814813394,   Reward: 17.675697466251812\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 27.222925652609618\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.37516705840351716,   Reward: 25.11766249471488\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 40.32639911272454\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.374255280954204,   Reward: 38.2211359548298\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 32.36390302845379\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.37334578010158254,   Reward: 30.258639870559055\n",
      "  Patch list: [1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 13.7623184991017\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.3724385501612696,   Reward: 9.551792183312227\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 9.050722189492937\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.37153358546307486,   Reward: 4.840195873703464\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0412  Accuracy: 9358/ 9500 (98.51%)\n",
      "\n",
      "Epoch time: 17.597153425216675\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 32.136796032870436\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.3702884436977972,   Reward: 26.873638138133593\n",
      "  Patch list: [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 18.185981848665953\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.36938884755227414,   Reward: 11.870192374981743\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 22\n",
      "  loss_reward: 7.955327200098929\n",
      "  patches_reward: -0.42105263157894735\n",
      "  Epsilon: 0.36849149758821825,   Reward: -0.4657254314800179\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 20.131136302754214\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3675963881971892,   Reward: 16.973241565912108\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 15.009263773768893\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3667035137847506,   Reward: 11.851369036926787\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.322461343944493\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3658128687704341,   Reward: 23.217198186049757\n",
      "  Patch list: [1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 12.129834492286939\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.3649244475877057,   Reward: 6.866676597550097\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 28.793067972556006\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.36403824468393015,   Reward: 26.68780481466127\n",
      "  Patch list: [1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 9.032600165445547\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.36315425452033634,   Reward: 2.716810691761337\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0364  Accuracy: 9382/ 9500 (98.76%)\n",
      "\n",
      "Epoch time: 17.657842874526978\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 17.487415004690703\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.361937971636616,   Reward: 14.329520267848597\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 33.81664272556554\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3610592255977531,   Reward: 30.658747988723437\n",
      "  Patch list: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 9.97716222804864\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.36018267368019313,   Reward: 4.714004333311798\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 63.677125866273556\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.35930831040548356,   Reward: 61.571862708378816\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 15.532536091069252\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.358436130308851,   Reward: 12.374641354227148\n",
      "  Patch list: [1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 12.602044043394125\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.3575661279391672,   Reward: 7.338886148657283\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 21.8673574629836\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3566982978589143,   Reward: 18.709462726141492\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 31.97084559628728\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.35583263464415177,   Reward: 28.812950859445174\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 15.666120497473825\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.35496913288448145,   Reward: 12.50822576063172\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0318  Accuracy: 9398/ 9500 (98.93%)\n",
      "\n",
      "Epoch time: 18.177029371261597\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 23.417579102099197\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.35378104001466976,   Reward: 13.943894891572882\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 176.643556561653\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.35292266083567925,   Reward: 174.53829340375827\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.29019371395319\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3520664249244353,   Reward: 23.184930556058454\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 38.98249173669892\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.35121232692946064,   Reward: 36.87722857880418\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.549339522499626\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.35036036151263983,   Reward: 23.44407636460489\n",
      "  Patch list: [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 7.254909419398534\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.3495105233491866,   Reward: -0.11351163323304458\n",
      "  Patch list: [1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 10.366457627433686\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.34866280712760944,   Reward: 6.1559313116442125\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 47.99144628987768\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3478172075496792,   Reward: 45.88618313198294\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 46.83061893524492\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.34697371933039584,   Reward: 44.72535577735018\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0347  Accuracy: 9390/ 9500 (98.84%)\n",
      "\n",
      "Epoch time: 17.44149398803711\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 31.608988984361137\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3458131631093138,   Reward: 29.5037258264664\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 193.22871729497282\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.34497467874370813,   Reward: 191.12345413707808\n",
      "  Patch list: [1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 7.210225311930024\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.344138287970935,   Reward: 4.052330575087919\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 29.552244303802066\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.34330398556354935,   Reward: 26.39434956695996\n",
      "  Patch list: [1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 9.274641273838215\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3424717663071584,   Reward: 6.11674653699611\n",
      "  Patch list: [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 26.856391689601594\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.3416416250003892,   Reward: 21.59323379486475\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 47.4566210923025\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3408135564548558,   Reward: 45.35135793440776\n",
      "  Patch list: [1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 7.346440719161577\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.3399875554951271,   Reward: 1.0306512454773662\n",
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 11.008135573579427\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3391636169586944,   Reward: 7.8502408367373215\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0349  Accuracy: 9389/ 9500 (98.83%)\n",
      "\n",
      "Epoch time: 16.035834312438965\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 18.879328868496145\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3380299591643288,   Reward: 16.77406571060141\n",
      "  Patch list: [0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 9.062026944216967\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.33721090850632934,   Reward: 2.7462374705327566\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 30.15443473180133\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3363939029175732,   Reward: 28.049171573906595\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 6.078832913405871\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.3355789372917727,   Reward: -3.394851297120444\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 16.449918691470852\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3347660065353901,   Reward: 13.292023954628746\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 69.5606337014043\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3339551055676056,   Reward: 67.45537054350956\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 24.533478073081156\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.3331462293202854,   Reward: 21.37558333623905\n",
      "  Patch list: [0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 8.626435356079574\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.33233937273795033,   Reward: 2.310645882395363\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 16.75075138429469\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.33153453077774425,   Reward: 14.645488226399955\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0331  Accuracy: 9398/ 9500 (98.93%)\n",
      "\n",
      "Epoch time: 13.609885454177856\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 20.804893325527566\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3304271479802718,   Reward: 18.69963016763283\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.376612658940374\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3296270806112341,   Reward: 23.271349501045638\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 17.577271936465245\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3288290109124907,   Reward: 15.472008778570508\n",
      "  Patch list: [1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Selected Patches: 25\n",
      "  loss_reward: 10.784289358958894\n",
      "  patches_reward: -0.2631578947368421\n",
      "  Epsilon: 0.3280329338961033,   Reward: 5.521131464222052\n",
      "  Patch list: [1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 17.773746285182938\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.32723884458658814,   Reward: 13.563219969393465\n",
      "  Patch list: [1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 23\n",
      "  loss_reward: 10.418071240356152\n",
      "  patches_reward: -0.3684210526315789\n",
      "  Epsilon: 0.32644673802088425,   Reward: 3.0496501877245734\n",
      "  Patch list: [0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 23.37919929682812\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3256566092483231,   Reward: 21.273936138933383\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 28.734461604270432\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.3248684533305972,   Reward: 24.52393528848096\n",
      "  Patch list: [1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "  Selected Patches: 26\n",
      "  loss_reward: 14.13585195143471\n",
      "  patches_reward: -0.21052631578947367\n",
      "  Epsilon: 0.3240822653417297,   Reward: 9.925325635645237\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0326  Accuracy: 9405/ 9500 (99.00%)\n",
      "\n",
      "Epoch time: 16.571561574935913\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Epoch: 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_297123/2790341334.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ViTnet.set_patches(torch.tensor(patch_probs).float())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 19.197363522979096\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3230005485606757,   Reward: 17.09210036508436\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 53.26982819338511\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3222190245013919,   Reward: 51.16456503549037\n",
      "  Patch list: [0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1]\n",
      "  Selected Patches: 24\n",
      "  loss_reward: 9.929696641108327\n",
      "  patches_reward: -0.3157894736842105\n",
      "  Epsilon: 0.3214394518120275,   Reward: 3.6139071674241166\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 13.901049760954729\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.3206618256202507,   Reward: 11.795786603059993\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 25.866512815669612\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.31988614106589536,   Reward: 23.761249657774876\n",
      "  Patch list: [1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "  Selected Patches: 21\n",
      "  loss_reward: 3.5541976647924405\n",
      "  patches_reward: -0.47368421052631576\n",
      "  Epsilon: 0.31911239330093044,   Reward: -5.919486545733875\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 22.468421281339538\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.31834057748942984,   Reward: 20.3631581234448\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 28\n",
      "  loss_reward: 17.58378918753271\n",
      "  patches_reward: -0.10526315789473684\n",
      "  Epsilon: 0.31757068880754225,   Reward: 15.478526029637973\n",
      "  Patch list: [0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "  Selected Patches: 27\n",
      "  loss_reward: 50.081143484794566\n",
      "  patches_reward: -0.15789473684210525\n",
      "  Epsilon: 0.31680272244346097,   Reward: 46.923248747952464\n",
      "\n",
      "Inizio Testing\n",
      "\n",
      "\n",
      "Average test loss: 0.0335  Accuracy: 9395/ 9500 (98.89%)\n",
      "\n",
      "Epoch time: 18.70443296432495\n",
      "########################################\n",
      "Episode End\n",
      "########################################\n",
      "Total Time: 886.6472690105438\n"
     ]
    }
   ],
   "source": [
    "initial = time.time()\n",
    "step_reward, selected_patch = model.train_test()\n",
    "rl_train_time = time.time() - initial\n",
    "print(f'Total Time: {rl_train_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLViT_FLOPs(nn.Module):\n",
    "    def __init__(self, vit_model, mask):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 임베딩\n",
    "        x = self.vit.to_patch_embedding(x)\n",
    "        pe = posemb_sincos_2d(x).to(x.device)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        # 2. Mask 적용\n",
    "        x = x[:, self.mask, :]\n",
    "\n",
    "        # 3. Transformer 적용\n",
    "        x = self.vit.transformer(x)\n",
    "\n",
    "        # 4. 평균 풀링\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # 5. Classification head\n",
    "        return self.vit.linear_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "\n",
    "def compute_flops(model, img_size=28):\n",
    "    dummy = torch.randn(1, 1, img_size, img_size).to(device)\n",
    "    macs, params = profile(model, inputs=(dummy,), verbose=False)\n",
    "    return macs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy (agent): 97.62%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# RL-ViT 최종 Accuracy\n",
    "# _, rl_acc = evaluate_agent(ViTnet, validation_loader, device, mode=\"agent\")\n",
    "_, rl_acc = evaluate_agent(\n",
    "    model=ViTnet,\n",
    "    agent=model.agent,\n",
    "    data_load=validation_loader,\n",
    "    device=device,\n",
    "    mode=\"agent\"\n",
    ")\n",
    "\n",
    "# RL-ViT FLOPs 계산\n",
    "final_mask = torch.tensor(ViTnet.get_patches()).bool()\n",
    "wrapper = RLViT_FLOPs(ViTnet, final_mask).to(device)\n",
    "rl_flops, rl_params = compute_flops(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGzCAYAAAD9pBdvAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIF0lEQVR4nO3deXRU5f0G8OfOTGYmySxJyA6BEPawJBggjQKiRKOlFNyIFgUjYKvgT0Rb5VhBrRUVaq1KxWIRl1pAK9LWymIEVEBZQthBNkkgGwGSyTrr+/tjkoGYhUwyMzfJPJ9z7pnhzl2+c0/qPL33XSQhhAARERGRTBRyF0BERET+jWGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIj82v3334/4+Hi5yyDyawwjRB3QypUrIUkSdu/eLXcpXUJBQQGeffZZ5Obmyl0KETVBJXcBRETeVlBQgOeeew7x8fFITk5u8Nny5cvhcDjkKYyIAPDOCBF1EVVVVW3aLyAgABqNxsPVEJE7GEaIOrG9e/fi1ltvhcFggE6nw/jx4/Hdd9812MZqteK5555Dv379oNVq0a1bN4wePRqbNm1ybVNUVISsrCz06NEDGo0GMTExmDRpEn788cer1vDVV19hzJgxCA4ORkhICCZNmoQjR464Pv/kk08gSRK2bt3aaN+3334bkiTh4MGDrnVHjx7FnXfeibCwMGi1WowYMQL//ve/G+xX/xhr69atePjhhxEZGYkePXo0Wd+WLVswcuRIAEBWVhYkSYIkSVi5ciWAxm1GfvzxR0iShCVLlmDp0qVISEhAUFAQbr75ZuTn50MIgT/84Q/o0aMHAgMDMWnSJFy8eLHReb/44gvXddHr9ZgwYQIOHTp01etJ5I/4mIaokzp06BDGjBkDg8GA3/3udwgICMDbb7+NcePGYevWrUhNTQUAPPvss1i0aBFmzpyJUaNGwWQyYffu3cjJycFNN90EALjjjjtw6NAhPPLII4iPj0dJSQk2bdqEvLy8Fht3fvnll7j11luRkJCAZ599FjU1NXjjjTdw3XXXIScnB/Hx8ZgwYQJ0Oh3WrFmD66+/vsH+q1evxuDBgzFkyBDXd7ruuuvQvXt3PPXUUwgODsaaNWswefJk/Otf/8Jtt93WYP+HH34YERERWLBgQbN3RgYNGoTnn38eCxYswIMPPogxY8YAAK699toWr+8//vEPWCwWPPLII7h48SJeeeUVTJkyBTfeeCO2bNmCJ598EidOnMAbb7yBJ554AitWrHDt+8EHH2D69OnIyMjAyy+/jOrqarz11lsYPXo09u7dywazRD8liKjDeffddwUAsWvXrma3mTx5slCr1eLkyZOudQUFBUKv14uxY8e61iUlJYkJEyY0e5xLly4JAGLx4sVu15mcnCwiIyPFhQsXXOv27dsnFAqFmDZtmmvdPffcIyIjI4XNZnOtKywsFAqFQjz//POudePHjxdDhw4VtbW1rnUOh0Nce+21ol+/fq519ddn9OjRDY7ZnF27dgkA4t1332302fTp00WvXr1c/z59+rQAICIiIkRZWZlr/fz58wUAkZSUJKxWa4PvplarXTVXVFSIkJAQMWvWrAbnKSoqEkajsdF6IhKCj2mIOiG73Y6NGzdi8uTJSEhIcK2PiYnBr371K3z77bcwmUwAgJCQEBw6dAjHjx9v8liBgYFQq9XYsmULLl261OoaCgsLkZubi/vvvx9hYWGu9cOGDcNNN92E//3vf651mZmZKCkpwZYtW1zrPvnkEzgcDmRmZgIALl68iK+++gpTpkxBRUUFSktLUVpaigsXLiAjIwPHjx/HuXPnGtQwa9YsKJXKVtfsjrvuugtGo9H17/o7Tffeey9UKlWD9RaLxVXbpk2bUFZWhnvuucf1HUpLS6FUKpGamorNmzd7pV6izoxhhKgTOn/+PKqrqzFgwIBGnw0aNAgOhwP5+fkAgOeffx5lZWXo378/hg4dit/+9rfYv3+/a3uNRoOXX34ZX3zxBaKiojB27Fi88sorKCoqarGGM2fOAECzNZSWlroendxyyy0wGo1YvXq1a5vVq1cjOTkZ/fv3BwCcOHECQgg888wziIiIaLAsXLgQAFBSUtLgPL17977qtWqrnj17Nvh3fTCJi4trcn19kKsPfTfeeGOj77Fx48ZG34GI2GaEqMsbO3YsTp48iXXr1mHjxo1455138Oc//xnLli3DzJkzAQBz587FxIkT8dlnn2HDhg145plnsGjRInz11VcYPnx4u2vQaDSYPHky1q5di7/+9a8oLi7Gtm3b8OKLL7q2qe9e+8QTTyAjI6PJ4/Tt27fBvwMDA9tdW3Oau+PS3HohBIDL3+ODDz5AdHR0o+2uvKtCRE78XwVRJxQREYGgoCAcO3as0WdHjx6FQqFo8P/gw8LCkJWVhaysLFRWVmLs2LF49tlnXWEEAPr06YPHH38cjz/+OI4fP47k5GT86U9/wocffthkDb169QKAZmsIDw9HcHCwa11mZibee+89ZGdn48iRIxBCuB7RAHA9bgoICEB6erqbV6RlkiR59Hgt6dOnDwAgMjLS49+DqKviYxqiTkipVOLmm2/GunXrGnS/LS4uxkcffYTRo0fDYDAAAC5cuNBgX51Oh759+8JsNgMAqqurUVtb22CbPn36QK/Xu7ZpSkxMDJKTk/Hee++hrKzMtf7gwYPYuHEjfv7znzfYPj09HWFhYVi9ejVWr16NUaNGNXjMEhkZiXHjxuHtt99GYWFho/OdP3++5YvSgvpQdGWd3pKRkQGDwYAXX3wRVqu10eft+R5EXRXvjBB1YCtWrMD69esbrX/00UfxwgsvYNOmTRg9ejQefvhhqFQqvP322zCbzXjllVdc2yYmJmLcuHFISUlBWFgYdu/ejU8++QRz5swBAPzwww8YP348pkyZgsTERKhUKqxduxbFxcW4++67W6xv8eLFuPXWW5GWloYZM2a4uvYajUY8++yzDbYNCAjA7bffjlWrVqGqqgpLlixpdLylS5di9OjRGDp0KGbNmoWEhAQUFxdjx44dOHv2LPbt29eGq+gMVyEhIVi2bBn0ej2Cg4ORmprqlTYnBoMBb731Fu677z5cc801uPvuuxEREYG8vDx8/vnnuO666/Dmm296/LxEnZrMvXmIqAn1XVebW/Lz84UQQuTk5IiMjAyh0+lEUFCQuOGGG8T27dsbHOuFF14Qo0aNEiEhISIwMFAMHDhQ/PGPfxQWi0UIIURpaamYPXu2GDhwoAgODhZGo1GkpqaKNWvWtKrWL7/8Ulx33XUiMDBQGAwGMXHiRHH48OEmt920aZMAICRJcn2Hnzp58qSYNm2aiI6OFgEBAaJ79+7iF7/4hfjkk08aXZ+Wuj7/1Lp160RiYqJQqVQNuvk217X3p12dN2/eLACIjz/+uMH65mrZvHmzyMjIEEajUWi1WtGnTx9x//33i927d7e6ZiJ/IQlR1+qKiIiISAZsM0JERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIiklWnGPTM4XCgoKAAer3ep8M6ExERUdsJIVBRUYHY2FgoFM3f/+gUYaSgoKDRTJlERETUOeTn56NHjx7Nft4pwoherwfg/DL1820QERFRx2YymRAXF+f6HW9Opwgj9Y9mDAYDwwgREVEnc7UmFmzASkRERLJiGCEiIiJZMYwQERGRrDpFmxEiImo7IQRsNhvsdrvcpVAXo1QqoVKp2j3sBsMIEVEXZrFYUFhYiOrqarlLoS4qKCgIMTExUKvVbT4GwwgRURflcDhw+vRpKJVKxMbGQq1Wc+BI8hghBCwWC86fP4/Tp0+jX79+LQ5s1hKGESKiLspiscDhcCAuLg5BQUFyl0NdUGBgIAICAnDmzBlYLBZotdo2HYcNWImIuri2/r9VotbwxN8X/0KJiIhIVgwjREREJCuGESIi6pLGjRuHuXPnuv4dHx+P1157rcV9JEnCZ5991u5ze+o4/oJhhIiIOpSJEyfilltuafKzb775BpIkYf/+/W4fd9euXXjwwQfbW14Dzz77LJKTkxutLywsxK233urRc/3UypUrERIS4tVz+Ipfh5EV357GM58dxPHiCrlLISKiOjNmzMCmTZtw9uzZRp+9++67GDFiBIYNG+b2cSMiInzWqyg6OhoajcYn5+oK/DqM/Gd/AT747gxOlVbJXQoRkU8IIVBtscmyCCFaVeMvfvELREREYOXKlQ3WV1ZW4uOPP8aMGTNw4cIF3HPPPejevTuCgoIwdOhQ/POf/2zxuD99THP8+HGMHTsWWq0WiYmJ2LRpU6N9nnzySfTv3x9BQUFISEjAM888A6vVCsB5Z+K5557Dvn37IEkSJEly1fzTxzQHDhzAjTfeiMDAQHTr1g0PPvggKisrXZ/ff//9mDx5MpYsWYKYmBh069YNs2fPdp2rLfLy8jBp0iTodDoYDAZMmTIFxcXFrs/37duHG264AXq9HgaDASkpKdi9ezcA4MyZM5g4cSJCQ0MRHByMwYMH43//+1+ba7kavx5nRKdxfv3KWpvMlRAR+UaN1Y7EBRtkOffh5zMQpL76z45KpcK0adOwcuVKPP30066B2j7++GPY7Xbcc889qKysREpKCp588kkYDAZ8/vnnuO+++9CnTx+MGjXqqudwOBy4/fbbERUVhe+//x7l5eUN2pfU0+v1WLlyJWJjY3HgwAHMmjULer0ev/vd75CZmYmDBw9i/fr1+PLLLwEARqOx0TGqqqqQkZGBtLQ07Nq1CyUlJZg5cybmzJnTIHBt3rwZMTEx2Lx5M06cOIHMzEwkJydj1qxZV/0+TX2/+iCydetW2Gw2zJ49G5mZmdiyZQsAYOrUqRg+fDjeeustKJVK5ObmIiAgAAAwe/ZsWCwWfP311wgODsbhw4eh0+ncrqO1/DqM6LV1YcTMMEJE1JE88MADWLx4MbZu3Ypx48YBcD6iueOOO2A0GmE0GvHEE0+4tn/kkUewYcMGrFmzplVh5Msvv8TRo0exYcMGxMbGAgBefPHFRu08fv/737vex8fH44knnsCqVavwu9/9DoGBgdDpdFCpVIiOjm72XB999BFqa2vx/vvvIzg4GADw5ptvYuLEiXj55ZcRFRUFAAgNDcWbb74JpVKJgQMHYsKECcjOzm5TGMnOzsaBAwdw+vRpxMXFAQDef/99DB48GLt27cLIkSORl5eH3/72txg4cCAAoF+/fq798/LycMcdd2Do0KEAgISEBLdrcIdfhxHXnRGGESLyE4EBShx+PkO2c7fWwIEDce2112LFihUYN24cTpw4gW+++QbPP/88AMBut+PFF1/EmjVrcO7cOVgsFpjN5la3CTly5Aji4uJcQQQA0tLSGm23evVqvP766zh58iQqKyths9lgMBha/T3qz5WUlOQKIgBw3XXXweFw4NixY64wMnjwYCiVl69RTEwMDhw44Na5rjxnXFycK4gAQGJiIkJCQnDkyBGMHDkS8+bNw8yZM/HBBx8gPT0dd911F/r06QMA+L//+z889NBD2LhxI9LT03HHHXe0qZ1Oa/l1mxG91nk7qoKPaYjIT0iShCC1SpbF3XlxZsyYgX/961+oqKjAu+++iz59+uD6668HACxevBh/+ctf8OSTT2Lz5s3Izc1FRkYGLBaLx67Vjh07MHXqVPz85z/Hf//7X+zduxdPP/20R89xpfpHJPUkSYLD4fDKuQBnT6BDhw5hwoQJ+Oqrr5CYmIi1a9cCAGbOnIlTp07hvvvuw4EDBzBixAi88cYbXqvFr8NI/Z2Ritq2NxAiIiLvmDJlChQKBT766CO8//77eOCBB1yBZtu2bZg0aRLuvfdeJCUlISEhAT/88EOrjz1o0CDk5+ejsLDQte67775rsM327dvRq1cvPP300xgxYgT69euHM2fONNhGrVbDbrdf9Vz79u1DVdXlzhLbtm2DQqHAgAEDWl2zO+q/X35+vmvd4cOHUVZWhsTERNe6/v3747HHHsPGjRtx++23491333V9FhcXh9/85jf49NNP8fjjj2P58uVeqRXw8zDCNiNERB2XTqdDZmYm5s+fj8LCQtx///2uz/r164dNmzZh+/btOHLkCH7961836ClyNenp6ejfvz+mT5+Offv24ZtvvsHTTz/dYJt+/fohLy8Pq1atwsmTJ/H666+77hzUi4+Px+nTp5Gbm4vS0lKYzeZG55o6dSq0Wi2mT5+OgwcPYvPmzXjkkUdw3333uR7RtJXdbkdubm6D5ciRI0hPT8fQoUMxdepU5OTkYOfOnZg2bRquv/56jBgxAjU1NZgzZw62bNmCM2fOYNu2bdi1axcGDRoEAJg7dy42bNiA06dPIycnB5s3b3Z95g1+HUbYm4aIqGObMWMGLl26hIyMjAbtO37/+9/jmmuuQUZGBsaNG4fo6GhMnjy51cdVKBRYu3YtampqMGrUKMycORN//OMfG2zzy1/+Eo899hjmzJmD5ORkbN++Hc8880yDbe644w7ccsstuOGGGxAREdFk9+KgoCBs2LABFy9exMiRI3HnnXdi/PjxePPNN927GE2orKzE8OHDGywTJ06EJElYt24dQkNDMXbsWKSnpyMhIQGrV68GACiVSly4cAHTpk1D//79MWXKFNx666147rnnADhDzuzZszFo0CDccsst6N+/P/7617+2u97mSKK1Hb9lZDKZYDQaUV5e7nbDoZZ8vr8Qsz/KwajeYVjz68YNl4iIOrPa2lqcPn0avXv3bvPU7kRX09LfWWt/v/37zoi2vs0I74wQERHJxb/DiKtrLxuwEhERycWvw4irASvvjBAREcnGr8PIlYOedYKmM0RERF2SX4eR+jsjVruA2ea9gWWIiOTE/7NF3uSJvy+/DiPBV0zYxEasRNTV1I/oWV1dLXMl1JXV/339dARZd/j13DQKhQSdRoVKsw2VZhsi9Bq5SyIi8hilUomQkBCUlJQAcI534e6Q7ETNEUKguroaJSUlCAkJaTCvjrv8OowAuBxGeGeEiLqg+tlk6wMJkaeFhIS0OGtxa/h9GNFrVSgyARXs3ktEXZAkSYiJiUFkZCSsVv53jjwrICCgXXdE6vl9GOHAZ0TkD5RKpUd+NIi8wa8bsAKcn4aIiEhufh9GOHMvERGRvPw+jFw58BkRERH5nt+HEb3W2S/aVMuGXURERHLw+zDCNiNERETy8vswwjYjRERE8vL7MMI7I0RERPLy+zBS32aE44wQERHJw+/DiGvQMz6mISIikgXDiKtrL3vTEBERycHvw4irASsf0xAREcmCYeSK3jRCCJmrISIi8j9+H0bqH9NY7QJmm0PmaoiIiPxPm8LI0qVLER8fD61Wi9TUVOzcubPZbVeuXAlJkhosWq22zQV7WrD68sTF7FFDRETke26HkdWrV2PevHlYuHAhcnJykJSUhIyMDJSUlDS7j8FgQGFhoWs5c+ZMu4r2JIVC4vw0REREMnI7jLz66quYNWsWsrKykJiYiGXLliEoKAgrVqxodh9JkhAdHe1aoqKi2lW0p3HgMyIiIvm4FUYsFgv27NmD9PT0ywdQKJCeno4dO3Y0u19lZSV69eqFuLg4TJo0CYcOHWrxPGazGSaTqcHiTfWNWCs4WR4REZHPuRVGSktLYbfbG93ZiIqKQlFRUZP7DBgwACtWrMC6devw4YcfwuFw4Nprr8XZs2ebPc+iRYtgNBpdS1xcnDtluo0DnxEREcnH671p0tLSMG3aNCQnJ+P666/Hp59+ioiICLz99tvN7jN//nyUl5e7lvz8fK/WyMc0RERE8lFdfZPLwsPDoVQqUVxc3GB9cXExoqOjW3WMgIAADB8+HCdOnGh2G41GA41G405p7cKZe4mIiOTj1p0RtVqNlJQUZGdnu9Y5HA5kZ2cjLS2tVcew2+04cOAAYmJi3KvUi/Qa52R5DCNERES+59adEQCYN28epk+fjhEjRmDUqFF47bXXUFVVhaysLADAtGnT0L17dyxatAgA8Pzzz+NnP/sZ+vbti7KyMixevBhnzpzBzJkzPftN2qG+zYiJDViJiIh8zu0wkpmZifPnz2PBggUoKipCcnIy1q9f72rUmpeXB4Xi8g2XS5cuYdasWSgqKkJoaChSUlKwfft2JCYmeu5btBPbjBAREclHEp1gQhaTyQSj0Yjy8nIYDAaPH/+db07hhc+PYFJyLP5y93CPH5+IiMgftfb32+/npgE4cy8REZGcGEYA6OoasHJuGiIiIt9jGAEHPSMiIpITwwiuaMBqZm8aIiIiX2MYAduMEBERyYlhBFdOlGdDJ+hcRERE1KUwjODyYxqbQ8Bsc8hcDRERkX9hGAEQrL489ht71BAREfkWwwgAhUK6ohErwwgREZEvMYzUYSNWIiIieTCM1Km/M1LByfKIiIh8imGkDgc+IyIikgfDSB3O3EtERCQPhpE6Bq1zfho2YCUiIvIthpE6bDNCREQkD4aROmwzQkREJA+GkTpsM0JERCQPhpE6rnFGeGeEiIjIpxhG6lw5WR4RERH5DsNIHZ2mrjcNwwgREZFPMYzUYQNWIiIieTCM1Lk8UR679hIREfkSw0gdAyfKIyIikgXDSB3dFQ1YhRAyV0NEROQ/GEbq1D+msTkEzDaHzNUQERH5D4aROsFqles9u/cSERH5DsNIHYVCuqIRK8MIERGRrzCMXOHywGfsUUNEROQrDCNX4Pw0REREvscwcgUOfEZEROR7DCNX4J0RIiIi32MYuYJB65yfhm1GiIiIfIdh5ArsTUNEROR7DCNXYJsRIiIi32MYuQLbjBAREfkew8gV6scZ4WMaIiIi32EYuYL+isnyiIiIyDcYRq6g0zh70/AxDRERke8wjFyBDViJiIh8j2HkCpe79nKcESIiIl9hGLmCgW1GiIiIfI5h5Ar1j2kqa20QQshcDRERkX9gGLlC/WMam0PAbHPIXA0REZF/YBi5QrBaBUlyvuejGiIiIt9gGLmCQiFBp+bAZ0RERL7EMPITru69nLmXiIjIJxhGfoLz0xAREfkWw8hPcOAzIiIi32IY+Qm9lkPCExER+RLDyE/oNWwzQkRE5EsMIz9xeUh43hkhIiLyBYaRn2CbESIiIt9iGPkJ9qYhIiLyrTaFkaVLlyI+Ph5arRapqanYuXNnq/ZbtWoVJEnC5MmT23Jan9BzsjwiIiKfcjuMrF69GvPmzcPChQuRk5ODpKQkZGRkoKSkpMX9fvzxRzzxxBMYM2ZMm4v1hfowwjYjREREvuF2GHn11Vcxa9YsZGVlITExEcuWLUNQUBBWrFjR7D52ux1Tp07Fc889h4SEhHYV7G06Dbv2EhER+ZJbYcRisWDPnj1IT0+/fACFAunp6dixY0ez+z3//POIjIzEjBkzWnUes9kMk8nUYPEVNmAlIiLyLbfCSGlpKex2O6Kiohqsj4qKQlFRUZP7fPvtt/j73/+O5cuXt/o8ixYtgtFodC1xcXHulNkulx/TcJwRIiIiX/Bqb5qKigrcd999WL58OcLDw1u93/z581FeXu5a8vPzvVhlQ5cHPeOdESIiIl9QubNxeHg4lEoliouLG6wvLi5GdHR0o+1PnjyJH3/8ERMnTnStczgczhOrVDh27Bj69OnTaD+NRgONRuNOaR5T/5imstYGIQQkSZKlDiIiIn/h1p0RtVqNlJQUZGdnu9Y5HA5kZ2cjLS2t0fYDBw7EgQMHkJub61p++ctf4oYbbkBubq5PH7+0Vv04IzaHgNnmkLkaIiKirs+tOyMAMG/ePEyfPh0jRozAqFGj8Nprr6GqqgpZWVkAgGnTpqF79+5YtGgRtFothgwZ0mD/kJAQAGi0vqMIVqsgSYAQzkc12gCl3CURERF1aW6HkczMTJw/fx4LFixAUVERkpOTsX79elej1ry8PCgUnXdgV4VCgk6tQoXZhopaKyL08jwuIiIi8heSEELIXcTVmEwmGI1GlJeXw2AweP18aYuyUVhei3/PuQ7DeoR4/XxERERdUWt/vzvvLQwv4vw0REREvsMw0gQOfEZEROQ7DCNN0GudQ8JzrBEiIiLvYxhpgt71mIajsBIREXkbw0gTXG1G+JiGiIjI6xhGmsA2I0RERL7DMNIEvZa9aYiIiHyFYaQJOk6WR0RE5DMMI01w3RnhYxoiIiKvYxhpgk7j7NrLxzRERETexzDSBDZgJSIi8h2GkSbUP6ap4DgjREREXscw0gQ9xxkhIiLyGYaRJuiu6NrbCSY1JiIi6tQYRppQ37XX5hAw2xwyV0NERNS1MYw0IVitgiQ533OsESIiIu9iGGmCQiFBp2YjViIiIl9gGGmGjgOfERER+QTDSDNcM/fyMQ0REZFXMYw0Q8+Bz4iIiHyCYaQZOq1zSHg2YCUiIvIuhpFmuAY+YwNWIiIir2IYaYaOo7ASERH5BMNIMzhZHhERkW8wjDTj8mR5DCNERETexDDSDHbtJSIi8g2GkWboOegZERGRTzCMNEOncXbt5Z0RIiIi72IYaQYHPSMiIvINhpFmuHrTcJwRIiIir2IYaYae44wQERH5BMNIM1yz9tbaIISQuRoiIqKui2GkGfq6uWlsDgGzzSFzNURERF0Xw0gzggKUkCTnexPbjRAREXkNw0gzFAoJOjUHPiMiIvI2hpEW6DjwGRERkdcxjLSAQ8ITERF5H8NIC+oHPjMxjBAREXkNw0gLdHU9aviYhoiIyHsYRlrgGviMvWmIiIi8hmGkBTqOwkpEROR1DCMt4GR5RERE3scw0oLLk+UxjBAREXkLw0gL2LWXiIjI+xhGWqDnoGdERERexzDSgvrJ8nhnhIiIyHsYRlpQ/5iGE+URERF5D8NICzg3DRERkfcxjLRAz3FGiIiIvI5hpAWuOyO1NgghZK6GiIioa2IYaUF9A1abQ6DW6pC5GiIioq6JYaQFQQFKSJLzfYWZjViJiIi8gWGkBQqFBJ2aA58RERF5U5vCyNKlSxEfHw+tVovU1FTs3Lmz2W0//fRTjBgxAiEhIQgODkZycjI++OCDNhfsa+xRQ0RE5F1uh5HVq1dj3rx5WLhwIXJycpCUlISMjAyUlJQ0uX1YWBiefvpp7NixA/v370dWVhaysrKwYcOGdhfvC3ot74wQERF5k9th5NVXX8WsWbOQlZWFxMRELFu2DEFBQVixYkWT248bNw633XYbBg0ahD59+uDRRx/FsGHD8O2337a7eF+4PPAZwwgREZE3uBVGLBYL9uzZg/T09MsHUCiQnp6OHTt2XHV/IQSys7Nx7NgxjB07ttntzGYzTCZTg0Uuuvoh4fmYhoiIyCvcCiOlpaWw2+2IiopqsD4qKgpFRUXN7ldeXg6dTge1Wo0JEybgjTfewE033dTs9osWLYLRaHQtcXFx7pTpUa6BzzgkPBERkVf4pDeNXq9Hbm4udu3ahT/+8Y+YN28etmzZ0uz28+fPR3l5uWvJz8/3RZlNqn9MU8HHNERERF6hcmfj8PBwKJVKFBcXN1hfXFyM6OjoZvdTKBTo27cvACA5ORlHjhzBokWLMG7cuCa312g00Gg07pTmNZEGZx1nL9XIXAkREVHX5NadEbVajZSUFGRnZ7vWORwOZGdnIy0trdXHcTgcMJvN7pxaNgOjDQCAI0XytVshIiLqyty6MwIA8+bNw/Tp0zFixAiMGjUKr732GqqqqpCVlQUAmDZtGrp3745FixYBcLb/GDFiBPr06QOz2Yz//e9/+OCDD/DWW2959pt4yaAYPQDgWFEFbHYHVEqOE0dERORJboeRzMxMnD9/HgsWLEBRURGSk5Oxfv16V6PWvLw8KBSXf7Crqqrw8MMP4+zZswgMDMTAgQPx4YcfIjMz03Pfwot6dQtGYIASNVY7frxQhb6RerlLIiIi6lIk0QmmozWZTDAajSgvL4fBYPD5+Scv3Ybc/DK8fs9w/DIp1ufnJyIi6oxa+/vNZw6tMCjGeQGPFrLdCBERkacxjLRCYl27kSMMI0RERB7HMNIK9XdGjhRWyFwJERFR18Mw0goD68JIkakWl6osMldDRETUtTCMtIJOo0LPsCAAfFRDRETkaQwjrTQw2tlu5DDDCBERkUcxjLQS240QERF5B8NIK10OI7wzQkRE5EkMI62UWBdGTpRUwmp3yFwNERFR18Ew0ko9QgOh06hgsTtw6nyV3OUQERF1GQwjraRQSK5GrHxUQ0RE5DkMI25guxEiIiLPYxhxQ30YYfdeIiIiz2EYccMg1xw17N5LRETkKQwjbhgQrYckAaWVZpyvMMtdDhERUZfAMOKGILUK8d2CAQBHi/iohoiIyBMYRtx0+VENwwgREZEnMIy4aVA0h4UnIiLyJIYRN7F7LxERkWcxjLhpUOzlYeHNNrvM1RAREXV+DCNuijVqYdCqYHMInCiplLscIiKiTo9hxE2SJF3xqIbtRoiIiNqLYaQN2G6EiIjIcxhG2qC+ey/HGiEiImo/hpE2uPIxjRBC5mqIiIg6N4aRNugfpYdCAi5WWVDCYeGJiIjahWGkDbQBSiRE6ABwBl8iIqL2YhhpIzZiJSIi8gyGkTa6PEcNu/cSERG1B8NIG/HOCBERkWcwjLRRYl0YOXW+ErVWDgtPRETUVgwjbRSp1yAsWA2HAI4Xc1h4IiKitmIYaSNJkjAwur7dCB/VEBERtRXDSDvUtxth914iIqK2YxhpBzZiJSIiaj+GkXa43L3XxGHhiYiI2ohhpB36RuqgUkgw1dpQUF4rdzlERESdEsNIO2hUSvSNdA4Lf6SAj2qIiIjagmGknerbjRwtYhghIiJqC4aRduKw8ERERO3DMNJO7FFDRETUPgwj7TQw2hlGTl+oQrXFJnM1REREnQ/DSDtF6DUI12kgBHCsiI9qiIiI3MUw4gFsN0JERNR2DCMekMh2I0RERG3GMOIBQ3sYAQBbfzgPh4MjsRIREbmDYcQDxg+Mgl6rQt7Fanx7olTucoiIiDoVhhEPCFQrccc1PQAA//j+jMzVEBERdS4MIx4yNbUnAODLIyUo4jw1RERErcYw4iH9ovQY1TsMdofA6l35cpdDRETUaTCMeFD93ZFVu/JgsztkroaIiKhzYBjxoFuGRCMsWI3C8lpsPnZe7nKIiIg6BYYRD9KolLhrBBuyEhERuYNhxMN+Ncr5qGbrD+eRf7Fa5mqIiIg6vjaFkaVLlyI+Ph5arRapqanYuXNns9suX74cY8aMQWhoKEJDQ5Gent7i9p1dr27BGNMvHEIA/9yZJ3c5REREHZ7bYWT16tWYN28eFi5ciJycHCQlJSEjIwMlJSVNbr9lyxbcc8892Lx5M3bs2IG4uDjcfPPNOHfuXLuL76impvYCAKzZnQ+LjQ1ZiYiIWiIJIdwavzw1NRUjR47Em2++CQBwOByIi4vDI488gqeeeuqq+9vtdoSGhuLNN9/EtGnTmtzGbDbDbDa7/m0ymRAXF4fy8nIYDAZ3ypWF1e7A6Je/QrHJjDd/NRy/GBYrd0lEREQ+ZzKZYDQar/r77dadEYvFgj179iA9Pf3yARQKpKenY8eOHa06RnV1NaxWK8LCwprdZtGiRTAaja4lLi7OnTJlF6BUIHOks+3IP77joxoiIqKWuBVGSktLYbfbERUV1WB9VFQUioqKWnWMJ598ErGxsQ0CzU/Nnz8f5eXlriU/v/MNInb3yDgoJGDHqQs4UVIpdzlEREQdlk9707z00ktYtWoV1q5dC61W2+x2Go0GBoOhwdLZxIYE4saBztDGhqxERETNcyuMhIeHQ6lUori4uMH64uJiREdHt7jvkiVL8NJLL2Hjxo0YNmyY+5V2QlN/5nxU88mes6i12mWuhoiIqGNyK4yo1WqkpKQgOzvbtc7hcCA7OxtpaWnN7vfKK6/gD3/4A9avX48RI0a0vdpOZmy/CPQIDUR5jRWf7y+UuxwiIqIOye3HNPPmzcPy5cvx3nvv4ciRI3jooYdQVVWFrKwsAMC0adMwf/581/Yvv/wynnnmGaxYsQLx8fEoKipCUVERKiu7fjsKpULCPXWDoHFEViIioqa5HUYyMzOxZMkSLFiwAMnJycjNzcX69etdjVrz8vJQWHj5LsBbb70Fi8WCO++8EzExMa5lyZIlnvsWHdiUEXFQKSTk5JXhcIFJ7nKIiIg6HLfHGZFDa/spd1Sz/5GDzw8U4t6f9cQLk4fKXQ4REZFPeGWcEWqbqanORzWf7S1AldnW4DOr3YEjhSZ8vDsfz/77EKYs24E5H+XAaufIrURE5B9UchfgD9L6dENCeDBOlVbhrS0nEW3U4lCBCYcKynG0qKLJIePHD4rEbcN7yFAtERGRbzGM+IAkSfhVak+88PkRvLn5RKPP9RoVEmMNGBxrxMUqMz7LLcDfvj6NycndIUmSDBUTERH5DsOIj9yVEoePduahrNqKwbEGDOluxJBYIwbHGtAzLAgKhTN0XKqyYMOhYhwpNGH7yQu4rm+4zJUTERF5F8OIjxiDAvDV4+Ouul1osBp3jeiB93ecwd++PsUwQkREXR4bsHZAM0b3hiQBW384jx+KK+Quh4iIyKsYRjqgXt2CkZHoHF7/nW9OyVwNERGRdzGMdFCzxiYAcHYHLqmolbkaIiIi72EY6aBSeoXimp4hsNgdeH87h5InIqKui2GkA3uw7u7IB9+dQbXFdpWtiYiIOieGkQ7spsRo9OoWhPIaKz7Zc1bucoiIiLyCYaQDUyokzBjdGwDwzjenYXd0+GmEiIiI3MYw0sHdmdIDxsAA5F2sxqbDRXKXQ0RE5HEMIx1ckFqF+37WCwDwt6/ZzZeIiLoehpFOYNq1vaBWKpCTV4Y9Zy7KXQ4REZFHMYx0ApF6LSYPjwUALP/6tMzVEBEReRbDSCcxc4yzm++Gw0U4c6FK5mqIiIg8h2Gkk+gfpce4AREQAvj7t7w7QkREXQfDSCfyYN3dkY93n8WlKovM1RAREXkGw0gnktanGxJjDKix2vGP7zlEPBERdQ0MI52IJEmuIeJXbj+DGotd5oqIiIjaj2Gkk5kwLAYxRi1KK834xRvfYM+ZS3KXRERE1C4MI51MgFKBV6ckI0KvwcnzVbhz2XY8959DnEiPiIg6LYaRTiitTzd8+dj1uDOlB4QA3t32IzJe+xrbTpTKXRoREZHbGEY6KWNQAJbclYT3HhiF7iGByL9Yg6nvfI+n/rUfplqr3OURERG1GsNIJ3d9/whseGysa/6aVbvycdOrW/Hl4WKZKyMiImodhpEuQKdR4Q+Th2D1gz9DfLcgFJvMmPn+bvzfP/fyLgkREXV4DCNdSGpCN6yfOxa/HpsAhQT8e18BnlizD0IIuUsjIiJqFsNIF6MNUGL+zwdh1YNpCFBK2Hi4GGv3npO7LCIiomYxjHRRo3qH4dHx/QAAC/99CIXlNTJXRERE1DSGkS7sN9f3QVJcCCpqbfjdJ/v5uIaIiDokhpEuTKVU4E93JUGjUuCb46X4x/d5cpdERETUCMNIF9c3UocnbxkIAHjxf0dw5kKVzBURERE1xDDiB+6/Nh4/SwhDtcWOJz7eB7uDj2uIiKjjYBjxAwqFhMV3JkGnUWHXj5fw929PyV0SERGRC8OIn4gLC8IzvxgEAFiy4Qf8UFwhc0VERERODCN+ZMqIONw4MBIWuwPz1uTCanfIXRIRERHDiD+RJAkv3T4UxsAAHDxnwtLNJ+QuiYiIiGHE30QatPjD5CEAgDe/OoEDZ8tlroiIiPwdw4gf+mVSLCYMi4HNITBvTS5qrXa5SyIiIj/GMOKn/jBpCMJ1GhwvqcTL64/KXQ4REfkxhhE/FRasxst3DAUAvLvtR6zL5WR6REQkD4YRPzZ+UBRm39AHAPC7T/bj4Dm2HyEiIt9jGPFz824agHEDImC2OfDrD/bgQqVZ7pKIiMjPMIz4OaVCwl/uHo7e4cE4V1aDOR/t5fgjRETkUwwjBGNgAP52XwqC1UrsOHUBL/7viNwlERGRH2EYIQBAvyg9/jQlGYCzQeu/9pyVtyAiIvIbDCPkcsuQaPzfjX0BAPPXHsD+s2XyFkRERH6BYYQamJveH+MHRsJS16D1fAUbtBIRkXcxjFADCoWEP9+djISIYBSW12L2Rzls0EpERF7FMEKNGLQB+Nt9I6DTqLDz9EW88N/DLW4vhEBFrRUWG0MLERG5TyV3AdQx9Y3U4c+ZyZj1/m68t+MMqix2BCglmGpsKK+xorzGClNt3WuNFQ4BROg1WDf7OsSGBMpdPhERdSKSEELIXcTVmEwmGI1GlJeXw2AwyF2OX/nLl8fx5y9/aPX21/bphg9npEKhkLxYFRERdQat/f1u052RpUuXYvHixSgqKkJSUhLeeOMNjBo1qsltDx06hAULFmDPnj04c+YM/vznP2Pu3LltOS3J4JEb+yJYo0RheS2MgQEwaFUwBgXUva97DQzApWoLblu6HdtPXsCKbacxc0yC3KUTEVEn4XYYWb16NebNm4dly5YhNTUVr732GjIyMnDs2DFERkY22r66uhoJCQm466678Nhjj3mkaPIdhUJqVbCIMmjx9IRB+P1nB/HKhmMY0y8CA6L1PqiQiIg6O7cbsL766quYNWsWsrKykJiYiGXLliEoKAgrVqxocvuRI0di8eLFuPvuu6HRaNpdMHVcU1N74sa6bsGPrtoLs83u9jFW78rDjUu2cBZhIiI/4lYYsVgs2LNnD9LT0y8fQKFAeno6duzY4bGizGYzTCZTg4U6PkmS8PIdw9AtWI2jRRV4dWPr25oAwHvbf8ST/zqAU6VVmLdmHzYcKvJSpURE1JG4FUZKS0tht9sRFRXVYH1UVBSKijz3w7Fo0SIYjUbXEhcX57Fjk3dF6DV46Y5hAIC/fXMK35260Kr93vnmFBb++xAAoF+kDnaHwCMf7cW2E6Veq5WIiDqGDjnOyPz581FeXu5a8vPz5S6J3HBTYhTuHhkHIYDH1+xDeY21xe3f2nISL3zunJxvzg198cWjY3DL4GhY7A7Men839uZd8kXZREQkE7fCSHh4OJRKJYqLixusLy4uRnR0tMeK0mg0MBgMDRbqXJ75RSJ6dQvCubIaLFx3sNnt3sg+jpfXHwUAzE3vh8dv7g+VUoG/3JOMMf3CUW2x4/53d+FYUYWvSiciIh9zK4yo1WqkpKQgOzvbtc7hcCA7OxtpaWkeL446r2CNCq9OSYZCAj7LLcB/9hU0+FwIgVc3/YA/bXK2K3ni5v6Ym94fkuQcn0SjUmLZvSkY3jME5TVW3Pv373HmQpXPvwcREXmf249p5s2bh+XLl+O9997DkSNH8NBDD6GqqgpZWVkAgGnTpmH+/Pmu7S0WC3Jzc5GbmwuLxYJz584hNzcXJ06c8Ny3oA4ppVco5tzYDwDw9NoDKCyvAeAMIos3HMPr2ccBAPNvHeja7krBGhVW3j8KA6P1OF9hxr1//x7FplrffQEiIvIJt8NIZmYmlixZggULFiA5ORm5ublYv369q1FrXl4eCgsLXdsXFBRg+PDhGD58OAoLC7FkyRIMHz4cM2fO9Ny3oA7rkRv7IqmHEaZaG574eB8cDoFFXxzFX7ecBAD8fsIg/Pr6Ps3ubwwKwPszRqFXtyDkX6zBve98j0tVFl+VT0REPsDh4MnrTp2vxITXv0WN1Y6kuBDsyy8DADw/aTCmpcW36hj5F6tx57LtKDaZkdTDiH/M+hl0Gk6tRETUkbX297tD9qahriUhQoenJwwCAFcQefG2oa0OIgAQFxaED2ekIjQoAPvOlmPWe7txodKMWqsdnSBPExFRC3hnhHxCCIFH/rkXGw8V44XJQzBlZNvGjtl/tgy/Wv49Ks021zpJArQqJQLVSmhVCmjVSgQGKKENUOJnCWGYc0M/BKqVnvoqRETUSq39/WYYIZ8RQqDaYkdwOx+vfHfqAuauykVRKxuzJoQHY/FdSUjpFdqu8xIRkXsYRqjLs9odqLXaUWO1w2x1oMZqR43F7lpXYjLjT5uOodhkhkICZo1NwGPp/aEN4F0SIiJfYBghAlBebcVz/z2ET3OcE+/1i9ThT1OSMKxHiLyFERH5ATZgJYKza/CrU5Lxt/tSEK7T4HhJJW7763b8aeMxWGwOucsjIiIwjJCfuHlwNDY+NhYTk2Jhdwi88dUJTFq6DYcLOCM0EZHc+JiG/M7n+wvxzLqDuFhlgUohYcrIOIQFqaFSSghQKhBQ96pSKhCgcL4PC1ZjdL9wBCiZ34mIWottRohacL7CjN9/dgAbDhVffeM6cWGB+M31fXBnSg9oVGwES0R0NQwjRFchhMDGw8XYdfoibA4Bq90Bm935anUIWG0O2BwOWOwCh86V40LdMPRRBg0eHNsH94yKQ5Cao8ASETWHYYTIg2osdqzalYe3t55yjW8SFqzGjNG9cV9aLxi0ATJXSETU8TCMEHmB2WbHpznn8NaWk8i7WA0A0GtVyLo2HlnX9UZosFrmCgGb3YEfL1TjWFEFqsw2TB7eHWoV27oQke8xjBB5kc3uwH/2F2Dp5pM4UVIJAFCrFDBoA6CQAIUkQap7BQCFApAgQamQMChGj3EDIjGufwQiDdo21yCEQEmFGUeLKnCsyFT3WoHjJZUNui3flBiFpb+6hoGEiHyOYYTIBxwOgQ2HivDm5hM41IZuwoNjDbhhQCTGDYhAclwIVM301qky2/BDcYUrcBytCx9l1dYmtw8MUKJ/tB5HCk2w2BxIHxSJpVOvYcNbIvIphhEiHxJC4HRpFcw2BxxCQAg4Fwg4BFzrzFY7vj99EVuOlWDf2fIGxzAGBmBMv3DcMCASgWoljhbW3e0orsCZC9VNnlchAfHhwRgYrceAKAMGxugxMFqPuNAgKBQSvv7hPGa9vxtmmwPjB0bir/cykBCR7zCMEHVw5yvM+PqH89jyw3l8/cN5lNc0fZejXrhOg0ExegyI0mNAtB4Dow3oF6W76lw73xw/j5nvOQPJDQMi8Na9KZyfh4h8gmGEqBOx2R3IzS/DlmPn8c3x8wCAAdF6DIg2YFC0M3x002nafPxtJ0ox471dqLU6MG5ABJYxkBCRDzCMEFED20+U4oG6QDK2fwT+dt/VA8mFSjPW7j2H9QeLoJAkRBo0iDJoEVX3Gqm//D5Y4xxzRQiBKosdphorKmptqKh1vprqXnUaFa7t2w2R+rY33iWizoFhhIga2XHyAh5YuQs1VjvG9AvH8mkjGgUSm92BLcfOY83ufHx1tAQ2R+v+E6HTqKCQgEqzDa3ZJTHGgOsHROD6/hG4pmcoe/sQdUEMI0TUpO9OXUDWu85AMrqvM5AEqpU4XlyBj/ecxac551BaaXZtn9TDiDtTeiA0WI1ikxklploUm2pRbDKjuKIWJSYzKs22RudRKSTotSrotQF1r873ReW1OHCuYeNdnUaFa/t0w9j+znASFxYEwHmXxWxzuO6wVJptqKy1wVRrg9XuQFqfbghvx+MrIvIuhhEiatbO0xdx/7s7UW2x45qeIXAIIDe/zPV5uE6N24Z3x10j4tA/Sn/V41WabSg21UIIwFAXOrQBCkh146z8VGmlGd8eL8XWusa79UPt14vUa2C1O1BptsFqb/4/UWqlAj8fGo370nrhmp6hzZ6vPYQQOFxoQq3VgZReoR4/PlFXxjBCRC3a9eNF3L9iJ6osdgDOOxk3DIzEXSk9cMPASJ/NUOxwCBwqMGHrDyXY+sN55OSVwf6T5zySBOjUKujq7rDoNCpUW+w4WlTh2mZgtB73pfXC5OTurvYr7ZF3oRrrcs/hs9xzOHm+CgAwZUQPLJw42CPHJ/IHDCNEdFU5eZfw1paTGBUfhsnDuyNCL/8jj/IaK06dr0Sw5nLwCFaroFA0vuuxL78MH353Bv/eVwBz3aizOo0Kd1zTHff+rBf6teKuzpVKK834fH8hPss9h715Za71apUCVrsDQgDx3YLw2t3DkRwX0p6vSeQXGEaIyG+UVVvwyZ6z+Mf3eThdWuVan9o7DMN7hkKnUSJY4ww2Oo0KwXWLTqNCkFqJ3Wcu4rO9Bfj2RKnrroxCAq7rG45fJsXiliHROFRgwrzVuSgor4VSIWHu+H54+Ia+UDYRkojIiWGEiPyOwyGw7WQpPthxBl8eKW5Vr56fSuphxC+Tu2PisJhGcweVV1vx9GcH8N/9hQCAkfGh+HNmMnqEBnmifKIuh2GEiPxaQVkN/rOvAMUmM6rMNlRanD1xqsw2VJptqHL9244eoYGYmBSLScmxSIjQtXhcIQTW7j2HBesOodJsg16jwgu3DcGk5O4++mZEnQfDCBGRF+VdqMbc1XuRU9e2ZHJyLJ6fPAQGbUCD7ewOgVqrHTVWO2qtdjgcQFxYoFd6/hB1NAwjREReZrM78ObmE3g9+zgcAggNCoAhMMAZPix21FodsNgdjfbr1S0IU0bE4fZruiPGGChD5US+wTBCROQje85cxNzVuci/WNPidhqVAkLAFVAUEjC2fwSmjIjD+EGRnFGZuhyGESIiH6qx2LHvbBlUCgnaACUC1Urna92iUSmgUEiottjwxYEirNmdj+9PX3TtHxoUgMnDu2PKiDgMinHvv3P18wGV11hhqrGivG4x1VghAHQPCURsSCBijNo2T5BosTnv8lhtDlgdDtjsAla7A9a6V5tdwOZwoGdYULsmdfQki82BgrIaVNQ62wlVW+pf7agyO9sLVVlsqLHY0T9Kh1uGxHSI7u1dCcMIEVEH92NpFT7ek49P9pxFsenyEPxDuxvROzwYNofzx95md8DmELDYnK+2uhBQbbE5Q0etrdFAcc0J12nQPTQQ3UO0iDUGontoIMKC1TDVWHGxyopL1RZcqrbgYpXz9VLduuq6wfFaI75bEK7pFYqUXqG4pmco+kfpW+wCXW2x4UhhBQ4XmnC4oByHC0wor7Eipq6+2BBnvd1DghAbokVsSKArVNkdAgVlNThdWtVoOXup2q0eVQoJGNU7DBOGxeKWwdEMJh7AMEJE1EnYHQJfHz+Pj3fnY9Ph4haHwG9JgFKCMdDZbsWgDYAxMAAO4fyxLiirRY219YGiJZIEBCgUCFBKUCkVCFA630sACsprG22v06gwvGcIrukZimt6hUICcLjQhEMFzvBxqrQK7v4ShevU0GsDcO5STZPtcuoFBigREhSAILWyblyZ+nFmnGPPBKuVUCkV2H7yAvZdMSWCQgJSe3fDz4fFMJi0A8MIEVEndLHKgo2HilBlsTt/7BUKqJSS6/2V64I1KlfoMAa2PB+QEAJl1VacK6vBubKauoDifH+pygpjYABCg9UICw5AaJAaoUFqhAWrERIU4HwNVEMT4AweLd3lKK+2Ym/+JeScuYScvDLszbvkmnKgJRF6DQbHGpAYY8DgWCO66dQoKq9tUO+5S873P71Lo1Yp0CssCL3Dg11LfHgwEsKDEaHXtLrnUv7FanxxsBCfHyhqNphkJEY1Gn+GmscwQkREsrM7BI4VVSAnzxlQ9uaXQQIwKNbgCh+JsQZE6lv3Ay+EgKnGhnNlNSivsaJH3WMcT4+E21wwkSQgpWcobhkSjYzB0a4ZplvjfIUZB86V4dT5KljsDjgcAnYHYBfC+b7u1Vb3bClcp0GPUOejqh4hgQjXaZqcFqEjYxghIiLygPpg8sXBogZzFgHAkO4G3DokBhmDo9E38vKAeWXVFhw4V479Z8ux/2wZDpwtb/IRljvUKgW6hwSie0igM6SEBMIYFACtSgmtWgmtSgFtgLJuUSCw7r3NIVBRa0VFrQ2mmrrX2oavFbVW/H5ColvhqjUYRoiIiDyssLwGGw8V44uDhdh5+mKDBrJ9I3XoF6nD4UITzlyobrSvJAF9I3QYEK2HNkAJlUKCQiFBKUlQKiQoJAkqpfNVQKC4/jHVpRoUmWrbNL2BO/71UBpSeoV59JgMI0RERF50odKMTYeLsf5QEbadKG3U8LhXtyAM6xGCYd2NGNbDiMHdjdBpVG06l9XuQFF5Lc7WtZs5e6kaBWU1qDTbXAPs1dqcg+2ZbQ7U1o34W2O1Q6VQQK91zoJt0AbUvW/4atCqcPPgaER5uD0MwwgREZGPlNdYsfloCYpNtUiMNWBodyNCgtRylyW71v5+ty2iERERkYsx0DloHbWNQu4CiIiIyL8xjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSVaeYtVcIAcA5FTERERF1DvW/2/W/483pFGGkoqICABAXFydzJUREROSuiooKGI3GZj+XxNXiSgfgcDhQUFAAvV4PSZI8dlyTyYS4uDjk5+fDYDB47LjUNF5v3+L19i1eb9/i9fattl5vIQQqKioQGxsLhaL5liGd4s6IQqFAjx49vHZ8g8HAP2Yf4vX2LV5v3+L19i1eb99qy/Vu6Y5IPTZgJSIiIlkxjBAREZGs/DqMaDQaLFy4EBqNRu5S/AKvt2/xevsWr7dv8Xr7lrevd6dowEpERERdl1/fGSEiIiL5MYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSlV+HkaVLlyI+Ph5arRapqanYuXOn3CV1CV9//TUmTpyI2NhYSJKEzz77rMHnQggsWLAAMTExCAwMRHp6Oo4fPy5PsV3AokWLMHLkSOj1ekRGRmLy5Mk4duxYg21qa2sxe/ZsdOvWDTqdDnfccQeKi4tlqrhze+uttzBs2DDXSJRpaWn44osvXJ/zWnvPSy+9BEmSMHfuXNc6Xm/PevbZZyFJUoNl4MCBrs+9db39NoysXr0a8+bNw8KFC5GTk4OkpCRkZGSgpKRE7tI6vaqqKiQlJWHp0qVNfv7KK6/g9ddfx7Jly/D9998jODgYGRkZqK2t9XGlXcPWrVsxe/ZsfPfdd9i0aROsVituvvlmVFVVubZ57LHH8J///Acff/wxtm7dioKCAtx+++0yVt159ejRAy+99BL27NmD3bt348Ybb8SkSZNw6NAhALzW3rJr1y68/fbbGDZsWIP1vN6eN3jwYBQWFrqWb7/91vWZ16638FOjRo0Ss2fPdv3bbreL2NhYsWjRIhmr6noAiLVr17r+7XA4RHR0tFi8eLFrXVlZmdBoNOKf//ynDBV2PSUlJQKA2Lp1qxDCeX0DAgLExx9/7NrmyJEjAoDYsWOHXGV2KaGhoeKdd97htfaSiooK0a9fP7Fp0yZx/fXXi0cffVQIwb9tb1i4cKFISkpq8jNvXm+/vDNisViwZ88epKenu9YpFAqkp6djx44dMlbW9Z0+fRpFRUUNrr3RaERqaiqvvYeUl5cDAMLCwgAAe/bsgdVqbXDNBw4ciJ49e/Kat5PdbseqVatQVVWFtLQ0XmsvmT17NiZMmNDgugL82/aW48ePIzY2FgkJCZg6dSry8vIAePd6d4pZez2ttLQUdrsdUVFRDdZHRUXh6NGjMlXlH4qKigCgyWtf/xm1ncPhwNy5c3HddddhyJAhAJzXXK1WIyQkpMG2vOZtd+DAAaSlpaG2thY6nQ5r165FYmIicnNzea09bNWqVcjJycGuXbsafca/bc9LTU3FypUrMWDAABQWFuK5557DmDFjcPDgQa9eb78MI0Rd1ezZs3Hw4MEGz3jJ8wYMGIDc3FyUl5fjk08+wfTp07F161a5y+py8vPz8eijj2LTpk3QarVyl+MXbr31Vtf7YcOGITU1Fb169cKaNWsQGBjotfP65WOa8PBwKJXKRi2Ai4uLER0dLVNV/qH++vLae96cOXPw3//+F5s3b0aPHj1c66Ojo2GxWFBWVtZge17ztlOr1ejbty9SUlKwaNEiJCUl4S9/+QuvtYft2bMHJSUluOaaa6BSqaBSqbB161a8/vrrUKlUiIqK4vX2spCQEPTv3x8nTpzw6t+3X4YRtVqNlJQUZGdnu9Y5HA5kZ2cjLS1Nxsq6vt69eyM6OrrBtTeZTPj+++957dtICIE5c+Zg7dq1+Oqrr9C7d+8Gn6ekpCAgIKDBNT927Bjy8vJ4zT3E4XDAbDbzWnvY+PHjceDAAeTm5rqWESNGYOrUqa73vN7eVVlZiZMnTyImJsa7f9/tav7aia1atUpoNBqxcuVKcfjwYfHggw+KkJAQUVRUJHdpnV5FRYXYu3ev2Lt3rwAgXn31VbF3715x5swZIYQQL730kggJCRHr1q0T+/fvF5MmTRK9e/cWNTU1MlfeOT300EPCaDSKLVu2iMLCQtdSXV3t2uY3v/mN6Nmzp/jqq6/E7t27RVpamkhLS5Ox6s7rqaeeElu3bhWnT58W+/fvF0899ZSQJEls3LhRCMFr7W1X9qYRgtfb0x5//HGxZcsWcfr0abFt2zaRnp4uwsPDRUlJiRDCe9fbb8OIEEK88cYbomfPnkKtVotRo0aJ7777Tu6SuoTNmzcLAI2W6dOnCyGc3XufeeYZERUVJTQajRg/frw4duyYvEV3Yk1dawDi3XffdW1TU1MjHn74YREaGiqCgoLEbbfdJgoLC+UruhN74IEHRK9evYRarRYRERFi/PjxriAiBK+1t/00jPB6e1ZmZqaIiYkRarVadO/eXWRmZooTJ064PvfW9ZaEEKJ991aIiIiI2s4v24wQERFRx8EwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWf0//56ylf0o0FkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_train = model.train_info()\n",
    "train_loss = results_train['train_loss']\n",
    "train_time = results_train['train_time']\n",
    "\n",
    "results_validation = model.validation_info()\n",
    "validation_loss = results_validation['validation_loss']\n",
    "validation_acc = results_validation['validation_acc']\n",
    "\n",
    "plt.plot(range(len(train_loss)), train_loss, label='Train Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss over time')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(validation_loss)), validation_loss, label='Validation Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Loss over time')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "x = range(len(validation_acc))\n",
    "y = validation_acc\n",
    "\n",
    "plt.plot(x, y, label='Validation Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Accuracy over time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch, random, numpy as np, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, data_load, device, mode = False):\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    elements = 0\n",
    "    csamp = 0\n",
    "    tloss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_load:\n",
    "\n",
    "            elements += len(data)\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            state = env.get_state(data)\n",
    "\n",
    "            if mode == \"agent\":\n",
    "              action = model.agent.select_action(state)\n",
    "            elif mode == \"random\":\n",
    "              action = model.env.action_space.sample()\n",
    "            else:\n",
    "              action = torch.tensor([1 for i in range(len(model.env.action_space.sample()))], dtype = torch.float)\n",
    "\n",
    "            ViTnet.set_patches(action)\n",
    "\n",
    "            output = functional.log_softmax(agent(data), dim=1)\n",
    "            loss = functional.nll_loss(output, target, reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            tloss += loss.item()\n",
    "            csamp += pred.eq(target).sum()\n",
    "\n",
    "    loss_val = tloss / elements\n",
    "    acc_val = (100.0 * csamp / elements).cpu()\n",
    "\n",
    "    print('\\nAverage validation loss: ' + '{:.4f}'.format(loss_val) +\n",
    "          '  Accuracy:' + '{:5}'.format(csamp) + '/' +\n",
    "          '{:5}'.format(elements) + ' (' +\n",
    "          '{:4.2f}'.format(acc_val) + '%)\\n')\n",
    "\n",
    "    return loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patch_mask_hard(dataloader, patch_log_dir=\"results/patch_logs_mnist\", num_samples=4, img_size=28,\n",
    "    patch_size=4, device=\"cuda\"):\n",
    "\n",
    "    classes = [str(i) for i in range(10)]   # MNIST 클래스명 0~9\n",
    "\n",
    "    # 데이터 샘플 가져오기\n",
    "    batch_data, batch_labels = next(iter(dataloader))\n",
    "    indices = random.sample(range(len(batch_data)), num_samples)\n",
    "    batch_data = batch_data[indices].to(device)\n",
    "    batch_labels = batch_labels[indices]\n",
    "\n",
    "    # JSON 로그 선택\n",
    "    all_logs = sorted([f for f in os.listdir(patch_log_dir) if f.endswith(\".json\")])\n",
    "    selected_logs = random.sample(all_logs, min(num_samples, len(all_logs)))\n",
    "\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(3 * num_samples, 6))\n",
    "\n",
    "    for i, (img, label, log_file) in enumerate(zip(batch_data, batch_labels, selected_logs)):\n",
    "        with open(os.path.join(patch_log_dir, log_file)) as f:\n",
    "            patch_log = json.load(f)\n",
    "\n",
    "        # patch_list 불러오기\n",
    "        patch_list = np.array(patch_log[\"patch_list\"], dtype=np.float32)\n",
    "\n",
    "        patch_probs = patch_list\n",
    "\n",
    "        n_patches = len(patch_probs)\n",
    "        grid_size = int(np.sqrt(n_patches))\n",
    "\n",
    "        hard_mask = (patch_probs > 0.5).astype(np.float32)\n",
    "        hard_mask = hard_mask.reshape(grid_size, grid_size)\n",
    "\n",
    "        # upsample mask → 이미지 크기와 맞추기\n",
    "        mask = np.repeat(np.repeat(hard_mask, patch_size, axis=0), patch_size, axis=1)\n",
    "\n",
    "        # MNIST: 1채널 → 2D를 3D로 확장\n",
    "        img_np = img.squeeze(0).cpu().numpy()    # (28,28)\n",
    "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
    "        img_np = np.repeat(img_np[..., None], 3, axis=2)   # (28,28,3)\n",
    "\n",
    "        masked_img = img_np * mask[..., None]\n",
    "\n",
    "        # 원본\n",
    "        axes[0, i].imshow(img_np, cmap=\"gray\")\n",
    "        axes[0, i].set_title(f\"Label: {classes[label.item()]}\")\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "        # 마스크\n",
    "        axes[1, i].imshow(masked_img, cmap=\"gray\")\n",
    "        axes[1, i].set_title(f\"Masked - {log_file}\")\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "        print(f\"{log_file} → probs range: {patch_probs.min():.3f} ~ {patch_probs.max():.3f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch049_batch0016.json → probs range: 0.000 ~ 1.000\n",
      "epoch113_batch0102.json → probs range: 0.000 ~ 1.000\n",
      "epoch001_batch0452.json → probs range: 0.000 ~ 1.000\n",
      "epoch040_batch0138.json → probs range: 0.000 ~ 1.000\n",
      "epoch049_batch0325.json → probs range: 0.000 ~ 1.000\n",
      "epoch032_batch0283.json → probs range: 0.000 ~ 1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 전처리 (normalize 포함)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))   # MNIST 공식값\n",
    "])\n",
    "\n",
    "val_dataset = datasets.MNIST(root=\"./Datasets_mnist\", train=False, download=True, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "visualize_patch_mask_hard(\n",
    "    dataloader=val_loader,\n",
    "    patch_log_dir=\"results/patch_logs_mnist\",\n",
    "    num_samples=6,\n",
    "    img_size=28,\n",
    "    patch_size=4,\n",
    "    device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6895210eb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "from torchinfo import summary\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_PATH = '/data/mnist'\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000\n",
    "\n",
    "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=True, download=True,\n",
    "                                       transform=transform_mnist)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(DOWNLOAD_PATH, train=False, download=True,\n",
    "                                      transform=transform_mnist)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to show a batch of images\n",
    "def show_batch(data_loader):\n",
    "    batch = next(iter(data_loader))\n",
    "    images, labels = batch\n",
    "    grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "    plt.title(\"Batch of Training Images\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print images for validation\n",
    "show_batch(train_loader)\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.ReLU(), #nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 4, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "ViT                                                3,264\n",
       "├─Sequential: 1-1                                  --\n",
       "│    └─Rearrange: 2-1                              --\n",
       "│    └─Linear: 2-2                                 1,088\n",
       "├─Dropout: 1-2                                     --\n",
       "├─Transformer: 1-3                                 --\n",
       "│    └─ModuleList: 2-3                             --\n",
       "│    │    └─ModuleList: 3-1                        82,432\n",
       "│    │    └─ModuleList: 3-2                        82,432\n",
       "│    │    └─ModuleList: 3-3                        82,432\n",
       "│    │    └─ModuleList: 3-4                        82,432\n",
       "│    │    └─ModuleList: 3-5                        82,432\n",
       "│    │    └─ModuleList: 3-6                        82,432\n",
       "├─Identity: 1-4                                    --\n",
       "├─Sequential: 1-5                                  --\n",
       "│    └─LayerNorm: 2-4                              128\n",
       "│    └─Linear: 2-5                                 650\n",
       "===========================================================================\n",
       "Total params: 499,722\n",
       "Trainable params: 499,722\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(image_size=28, patch_size=4, num_classes=10, channels=1, dim=64, depth=6, heads=4, mlp_dim=128)\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499722\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, data_loader, loss_history):\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (data, target) in enumerate(data_loader):\n",
    "        batch_start = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data.to(device)), dim=1)\n",
    "        loss = F.nll_loss(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            batch_time = time.time() - batch_start\n",
    "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
    "                  f' ({100 * i / len(data_loader):3.0f}%)]  Loss: {loss.item():6.4f}  Batch time: {batch_time:.4f}s')\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch training time: {epoch_time:.2f} seconds\")\n",
    "\n",
    "    return epoch_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = F.log_softmax(model(data.to(device)), dim=1)\n",
    "            loss = F.nll_loss(output, target.to(device), reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target.to(device)).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    eval_time = time.time() - start_time\n",
    "\n",
    "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
    "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
    "          '{:5}'.format(total_samples) + ' (' +\n",
    "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)')\n",
    "    \n",
    "    print(f\"Evaluation time: {eval_time:.2f} seconds\\n\")\n",
    "\n",
    "    return eval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 2.4061  Batch time: 0.0279s\n",
      "[10000/60000 ( 17%)]  Loss: 0.5019  Batch time: 0.0202s\n",
      "[20000/60000 ( 33%)]  Loss: 0.2695  Batch time: 0.0216s\n",
      "[30000/60000 ( 50%)]  Loss: 0.2921  Batch time: 0.0215s\n",
      "[40000/60000 ( 67%)]  Loss: 0.3014  Batch time: 0.0230s\n",
      "[50000/60000 ( 83%)]  Loss: 0.2756  Batch time: 0.0238s\n",
      "Epoch training time: 26.07 seconds\n",
      "\n",
      "Average test loss: 0.2013  Accuracy: 9355/10000 (93.55%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.1726  Batch time: 0.0223s\n",
      "[10000/60000 ( 17%)]  Loss: 0.1711  Batch time: 0.0240s\n",
      "[20000/60000 ( 33%)]  Loss: 0.1594  Batch time: 0.0222s\n",
      "[30000/60000 ( 50%)]  Loss: 0.1795  Batch time: 0.0234s\n",
      "[40000/60000 ( 67%)]  Loss: 0.1358  Batch time: 0.0234s\n",
      "[50000/60000 ( 83%)]  Loss: 0.2411  Batch time: 0.0208s\n",
      "Epoch training time: 26.05 seconds\n",
      "\n",
      "Average test loss: 0.1468  Accuracy: 9550/10000 (95.50%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.0675  Batch time: 0.0157s\n",
      "[10000/60000 ( 17%)]  Loss: 0.1874  Batch time: 0.0199s\n",
      "[20000/60000 ( 33%)]  Loss: 0.1861  Batch time: 0.0191s\n",
      "[30000/60000 ( 50%)]  Loss: 0.1832  Batch time: 0.0229s\n",
      "[40000/60000 ( 67%)]  Loss: 0.1551  Batch time: 0.0202s\n",
      "[50000/60000 ( 83%)]  Loss: 0.1465  Batch time: 0.0244s\n",
      "Epoch training time: 26.03 seconds\n",
      "\n",
      "Average test loss: 0.1207  Accuracy: 9639/10000 (96.39%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.1064  Batch time: 0.0235s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0558  Batch time: 0.0232s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0813  Batch time: 0.0214s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0994  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0761  Batch time: 0.0215s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0868  Batch time: 0.0245s\n",
      "Epoch training time: 26.08 seconds\n",
      "\n",
      "Average test loss: 0.1117  Accuracy: 9669/10000 (96.69%)\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.0660  Batch time: 0.0238s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0814  Batch time: 0.0237s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0656  Batch time: 0.0209s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0421  Batch time: 0.0224s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0905  Batch time: 0.0226s\n",
      "[50000/60000 ( 83%)]  Loss: 0.1613  Batch time: 0.0241s\n",
      "Epoch training time: 25.87 seconds\n",
      "\n",
      "Average test loss: 0.1210  Accuracy: 9610/10000 (96.10%)\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.1042  Batch time: 0.0132s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0707  Batch time: 0.0127s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0666  Batch time: 0.0223s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0783  Batch time: 0.0213s\n",
      "[40000/60000 ( 67%)]  Loss: 0.1153  Batch time: 0.0225s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0299  Batch time: 0.0233s\n",
      "Epoch training time: 24.42 seconds\n",
      "\n",
      "Average test loss: 0.1061  Accuracy: 9679/10000 (96.79%)\n",
      "Evaluation time: 2.12 seconds\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.0557  Batch time: 0.0257s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0867  Batch time: 0.0275s\n",
      "[20000/60000 ( 33%)]  Loss: 0.1299  Batch time: 0.0270s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0804  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0870  Batch time: 0.0225s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0406  Batch time: 0.0231s\n",
      "Epoch training time: 27.66 seconds\n",
      "\n",
      "Average test loss: 0.0894  Accuracy: 9712/10000 (97.12%)\n",
      "Evaluation time: 1.88 seconds\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.0447  Batch time: 0.0222s\n",
      "[10000/60000 ( 17%)]  Loss: 0.1446  Batch time: 0.0247s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0448  Batch time: 0.0220s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0798  Batch time: 0.0217s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0871  Batch time: 0.0242s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0338  Batch time: 0.0232s\n",
      "Epoch training time: 26.07 seconds\n",
      "\n",
      "Average test loss: 0.0888  Accuracy: 9706/10000 (97.06%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.0995  Batch time: 0.0208s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0196  Batch time: 0.0231s\n",
      "[20000/60000 ( 33%)]  Loss: 0.1222  Batch time: 0.0220s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0882  Batch time: 0.0220s\n",
      "[40000/60000 ( 67%)]  Loss: 0.1914  Batch time: 0.0224s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0071  Batch time: 0.0233s\n",
      "Epoch training time: 25.95 seconds\n",
      "\n",
      "Average test loss: 0.0951  Accuracy: 9708/10000 (97.08%)\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0460  Batch time: 0.0220s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0146  Batch time: 0.0213s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0505  Batch time: 0.0209s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0712  Batch time: 0.0279s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0445  Batch time: 0.0247s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0837  Batch time: 0.0225s\n",
      "Epoch training time: 27.23 seconds\n",
      "\n",
      "Average test loss: 0.0753  Accuracy: 9761/10000 (97.61%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 11\n",
      "[    0/60000 (  0%)]  Loss: 0.0478  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0422  Batch time: 0.0255s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0233  Batch time: 0.0245s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0648  Batch time: 0.0261s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0156  Batch time: 0.0251s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0406  Batch time: 0.0238s\n",
      "Epoch training time: 29.19 seconds\n",
      "\n",
      "Average test loss: 0.0719  Accuracy: 9780/10000 (97.80%)\n",
      "Evaluation time: 2.31 seconds\n",
      "\n",
      "Epoch: 12\n",
      "[    0/60000 (  0%)]  Loss: 0.0148  Batch time: 0.0234s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0118  Batch time: 0.0235s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0589  Batch time: 0.0215s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0567  Batch time: 0.0215s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0420  Batch time: 0.0244s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0583  Batch time: 0.0225s\n",
      "Epoch training time: 27.11 seconds\n",
      "\n",
      "Average test loss: 0.0798  Accuracy: 9758/10000 (97.58%)\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 13\n",
      "[    0/60000 (  0%)]  Loss: 0.0997  Batch time: 0.0232s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0595  Batch time: 0.0234s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0399  Batch time: 0.0243s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0672  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0343  Batch time: 0.0274s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0566  Batch time: 0.0267s\n",
      "Epoch training time: 27.66 seconds\n",
      "\n",
      "Average test loss: 0.0752  Accuracy: 9780/10000 (97.80%)\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 14\n",
      "[    0/60000 (  0%)]  Loss: 0.0272  Batch time: 0.0278s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0418  Batch time: 0.0276s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0757  Batch time: 0.0262s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0565  Batch time: 0.0289s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0165  Batch time: 0.0217s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0436  Batch time: 0.0214s\n",
      "Epoch training time: 28.25 seconds\n",
      "\n",
      "Average test loss: 0.0632  Accuracy: 9787/10000 (97.87%)\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 15\n",
      "[    0/60000 (  0%)]  Loss: 0.0115  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0231  Batch time: 0.0219s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0519  Batch time: 0.0219s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0057  Batch time: 0.0213s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0665  Batch time: 0.0223s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0402  Batch time: 0.0220s\n",
      "Epoch training time: 26.09 seconds\n",
      "\n",
      "Average test loss: 0.0571  Accuracy: 9821/10000 (98.21%)\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 16\n",
      "[    0/60000 (  0%)]  Loss: 0.0555  Batch time: 0.0234s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0415  Batch time: 0.0200s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0035  Batch time: 0.0224s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0133  Batch time: 0.0236s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0388  Batch time: 0.0217s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0408  Batch time: 0.0235s\n",
      "Epoch training time: 26.08 seconds\n",
      "\n",
      "Average test loss: 0.0715  Accuracy: 9788/10000 (97.88%)\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 17\n",
      "[    0/60000 (  0%)]  Loss: 0.0177  Batch time: 0.0275s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0355  Batch time: 0.0230s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0081  Batch time: 0.0226s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0291  Batch time: 0.0215s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0373  Batch time: 0.0232s\n",
      "[50000/60000 ( 83%)]  Loss: 0.1449  Batch time: 0.0219s\n",
      "Epoch training time: 26.05 seconds\n",
      "\n",
      "Average test loss: 0.0739  Accuracy: 9795/10000 (97.95%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 18\n",
      "[    0/60000 (  0%)]  Loss: 0.0565  Batch time: 0.0226s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0263  Batch time: 0.0237s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0327  Batch time: 0.0253s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0481  Batch time: 0.0231s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0175  Batch time: 0.0237s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0339  Batch time: 0.0224s\n",
      "Epoch training time: 26.26 seconds\n",
      "\n",
      "Average test loss: 0.0545  Accuracy: 9831/10000 (98.31%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 19\n",
      "[    0/60000 (  0%)]  Loss: 0.0157  Batch time: 0.0226s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0018  Batch time: 0.0231s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0247  Batch time: 0.0243s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0335  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0306  Batch time: 0.0198s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0569  Batch time: 0.0236s\n",
      "Epoch training time: 26.23 seconds\n",
      "\n",
      "Average test loss: 0.0774  Accuracy: 9776/10000 (97.76%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 20\n",
      "[    0/60000 (  0%)]  Loss: 0.1704  Batch time: 0.0227s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0186  Batch time: 0.0226s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0637  Batch time: 0.0276s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0649  Batch time: 0.0189s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0168  Batch time: 0.0273s\n",
      "[50000/60000 ( 83%)]  Loss: 0.1358  Batch time: 0.0273s\n",
      "Epoch training time: 28.77 seconds\n",
      "\n",
      "Average test loss: 0.0771  Accuracy: 9770/10000 (97.70%)\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 21\n",
      "[    0/60000 (  0%)]  Loss: 0.0133  Batch time: 0.0271s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0494  Batch time: 0.0299s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0593  Batch time: 0.0267s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0092  Batch time: 0.0264s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0672  Batch time: 0.0274s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0578  Batch time: 0.0267s\n",
      "Epoch training time: 29.46 seconds\n",
      "\n",
      "Average test loss: 0.0709  Accuracy: 9797/10000 (97.97%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 22\n",
      "[    0/60000 (  0%)]  Loss: 0.0123  Batch time: 0.0205s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0175  Batch time: 0.0228s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0063  Batch time: 0.0242s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0160  Batch time: 0.0217s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0289  Batch time: 0.0229s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0254  Batch time: 0.0230s\n",
      "Epoch training time: 25.81 seconds\n",
      "\n",
      "Average test loss: 0.0666  Accuracy: 9815/10000 (98.15%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 23\n",
      "[    0/60000 (  0%)]  Loss: 0.0472  Batch time: 0.0249s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0182  Batch time: 0.0237s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0279  Batch time: 0.0231s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0133  Batch time: 0.0233s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0026  Batch time: 0.0196s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0025  Batch time: 0.0247s\n",
      "Epoch training time: 25.81 seconds\n",
      "\n",
      "Average test loss: 0.0746  Accuracy: 9793/10000 (97.93%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 24\n",
      "[    0/60000 (  0%)]  Loss: 0.0985  Batch time: 0.0149s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0428  Batch time: 0.0246s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0482  Batch time: 0.0203s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0191  Batch time: 0.0224s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0374  Batch time: 0.0246s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0102  Batch time: 0.0201s\n",
      "Epoch training time: 26.16 seconds\n",
      "\n",
      "Average test loss: 0.0599  Accuracy: 9813/10000 (98.13%)\n",
      "Evaluation time: 2.30 seconds\n",
      "\n",
      "Epoch: 25\n",
      "[    0/60000 (  0%)]  Loss: 0.0096  Batch time: 0.0235s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0067  Batch time: 0.0267s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0051  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0464  Batch time: 0.0212s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0086  Batch time: 0.0198s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0317  Batch time: 0.0227s\n",
      "Epoch training time: 27.00 seconds\n",
      "\n",
      "Average test loss: 0.0670  Accuracy: 9805/10000 (98.05%)\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 26\n",
      "[    0/60000 (  0%)]  Loss: 0.0229  Batch time: 0.0223s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0630  Batch time: 0.0223s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0283  Batch time: 0.0189s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0705  Batch time: 0.0238s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0036  Batch time: 0.0235s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0113  Batch time: 0.0229s\n",
      "Epoch training time: 25.96 seconds\n",
      "\n",
      "Average test loss: 0.0804  Accuracy: 9788/10000 (97.88%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 27\n",
      "[    0/60000 (  0%)]  Loss: 0.0762  Batch time: 0.0242s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0375  Batch time: 0.0252s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0021  Batch time: 0.0231s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0371  Batch time: 0.0233s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0072  Batch time: 0.0344s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0049  Batch time: 0.0216s\n",
      "Epoch training time: 26.15 seconds\n",
      "\n",
      "Average test loss: 0.0772  Accuracy: 9788/10000 (97.88%)\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 28\n",
      "[    0/60000 (  0%)]  Loss: 0.0795  Batch time: 0.0236s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0012  Batch time: 0.0239s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0035  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0020  Batch time: 0.0222s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0552  Batch time: 0.0216s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0147  Batch time: 0.0234s\n",
      "Epoch training time: 26.12 seconds\n",
      "\n",
      "Average test loss: 0.0691  Accuracy: 9801/10000 (98.01%)\n",
      "Evaluation time: 2.30 seconds\n",
      "\n",
      "Epoch: 29\n",
      "[    0/60000 (  0%)]  Loss: 0.0266  Batch time: 0.0223s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0169  Batch time: 0.0255s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0036  Batch time: 0.0219s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0004  Batch time: 0.0260s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0050  Batch time: 0.0261s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0518  Batch time: 0.0250s\n",
      "Epoch training time: 29.57 seconds\n",
      "\n",
      "Average test loss: 0.0759  Accuracy: 9804/10000 (98.04%)\n",
      "Evaluation time: 2.32 seconds\n",
      "\n",
      "Epoch: 30\n",
      "[    0/60000 (  0%)]  Loss: 0.0497  Batch time: 0.0250s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0116  Batch time: 0.0234s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0126  Batch time: 0.0249s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0083  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0033  Batch time: 0.0243s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0128  Batch time: 0.0258s\n",
      "Epoch training time: 29.48 seconds\n",
      "\n",
      "Average test loss: 0.0730  Accuracy: 9796/10000 (97.96%)\n",
      "Evaluation time: 2.30 seconds\n",
      "\n",
      "Epoch: 31\n",
      "[    0/60000 (  0%)]  Loss: 0.0102  Batch time: 0.0256s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0814  Batch time: 0.0248s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0028  Batch time: 0.0247s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0091  Batch time: 0.0237s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0220  Batch time: 0.0241s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0305  Batch time: 0.0243s\n",
      "Epoch training time: 29.54 seconds\n",
      "\n",
      "Average test loss: 0.0696  Accuracy: 9797/10000 (97.97%)\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 32\n",
      "[    0/60000 (  0%)]  Loss: 0.0617  Batch time: 0.0216s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0273  Batch time: 0.0238s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0330  Batch time: 0.0217s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0047  Batch time: 0.0247s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0302  Batch time: 0.0235s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0032  Batch time: 0.0219s\n",
      "Epoch training time: 26.01 seconds\n",
      "\n",
      "Average test loss: 0.0690  Accuracy: 9815/10000 (98.15%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 33\n",
      "[    0/60000 (  0%)]  Loss: 0.0298  Batch time: 0.0216s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0224  Batch time: 0.0242s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0322  Batch time: 0.0226s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0068  Batch time: 0.0232s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0230  Batch time: 0.0244s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0217  Batch time: 0.0233s\n",
      "Epoch training time: 26.25 seconds\n",
      "\n",
      "Average test loss: 0.0913  Accuracy: 9750/10000 (97.50%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 34\n",
      "[    0/60000 (  0%)]  Loss: 0.0253  Batch time: 0.0231s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0285  Batch time: 0.0232s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0127  Batch time: 0.0242s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0037  Batch time: 0.0221s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0007  Batch time: 0.0215s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0111  Batch time: 0.0249s\n",
      "Epoch training time: 26.04 seconds\n",
      "\n",
      "Average test loss: 0.0551  Accuracy: 9848/10000 (98.48%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 35\n",
      "[    0/60000 (  0%)]  Loss: 0.0070  Batch time: 0.0233s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0003  Batch time: 0.0214s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0182  Batch time: 0.0230s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0250  Batch time: 0.0226s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0011  Batch time: 0.0242s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0215  Batch time: 0.0260s\n",
      "Epoch training time: 26.23 seconds\n",
      "\n",
      "Average test loss: 0.0649  Accuracy: 9834/10000 (98.34%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 36\n",
      "[    0/60000 (  0%)]  Loss: 0.0208  Batch time: 0.0215s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0032  Batch time: 0.0222s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0020  Batch time: 0.0219s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0070  Batch time: 0.0224s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0125  Batch time: 0.0227s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0012  Batch time: 0.0222s\n",
      "Epoch training time: 26.12 seconds\n",
      "\n",
      "Average test loss: 0.0861  Accuracy: 9767/10000 (97.67%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 37\n",
      "[    0/60000 (  0%)]  Loss: 0.0258  Batch time: 0.0207s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0034  Batch time: 0.0218s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0527  Batch time: 0.0230s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0376  Batch time: 0.0220s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0065  Batch time: 0.0213s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0299  Batch time: 0.0235s\n",
      "Epoch training time: 25.82 seconds\n",
      "\n",
      "Average test loss: 0.0598  Accuracy: 9834/10000 (98.34%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 38\n",
      "[    0/60000 (  0%)]  Loss: 0.0016  Batch time: 0.0216s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0090  Batch time: 0.0221s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0072  Batch time: 0.0238s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0599  Batch time: 0.0228s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0007  Batch time: 0.0241s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0614  Batch time: 0.0221s\n",
      "Epoch training time: 25.98 seconds\n",
      "\n",
      "Average test loss: 0.0703  Accuracy: 9828/10000 (98.28%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 39\n",
      "[    0/60000 (  0%)]  Loss: 0.0189  Batch time: 0.0233s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0084  Batch time: 0.0215s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0247  Batch time: 0.0206s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0043  Batch time: 0.0231s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0031  Batch time: 0.0223s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0013  Batch time: 0.0236s\n",
      "Epoch training time: 25.90 seconds\n",
      "\n",
      "Average test loss: 0.0560  Accuracy: 9844/10000 (98.44%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 40\n",
      "[    0/60000 (  0%)]  Loss: 0.0089  Batch time: 0.0219s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0075  Batch time: 0.0230s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0061  Batch time: 0.0213s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0067  Batch time: 0.0247s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0025  Batch time: 0.0240s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0086  Batch time: 0.0278s\n",
      "Epoch training time: 25.67 seconds\n",
      "\n",
      "Average test loss: 0.0540  Accuracy: 9845/10000 (98.45%)\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 41\n",
      "[    0/60000 (  0%)]  Loss: 0.0014  Batch time: 0.0232s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0229  Batch time: 0.0230s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0013  Batch time: 0.0201s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0280  Batch time: 0.0202s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0142  Batch time: 0.0206s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0394  Batch time: 0.0238s\n",
      "Epoch training time: 26.08 seconds\n",
      "\n",
      "Average test loss: 0.0782  Accuracy: 9801/10000 (98.01%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 42\n",
      "[    0/60000 (  0%)]  Loss: 0.0748  Batch time: 0.0227s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0093  Batch time: 0.0241s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0018  Batch time: 0.0223s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0033  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0261  Batch time: 0.0220s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0029  Batch time: 0.0228s\n",
      "Epoch training time: 25.79 seconds\n",
      "\n",
      "Average test loss: 0.0622  Accuracy: 9845/10000 (98.45%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 43\n",
      "[    0/60000 (  0%)]  Loss: 0.0073  Batch time: 0.0236s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0025  Batch time: 0.0219s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0047  Batch time: 0.0205s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0089  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0006  Batch time: 0.0219s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0183  Batch time: 0.0241s\n",
      "Epoch training time: 26.22 seconds\n",
      "\n",
      "Average test loss: 0.0641  Accuracy: 9819/10000 (98.19%)\n",
      "Evaluation time: 1.99 seconds\n",
      "\n",
      "Epoch: 44\n",
      "[    0/60000 (  0%)]  Loss: 0.0011  Batch time: 0.0231s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0219  Batch time: 0.0238s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0059  Batch time: 0.0222s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0036  Batch time: 0.0244s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0011  Batch time: 0.0225s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0046  Batch time: 0.0224s\n",
      "Epoch training time: 26.29 seconds\n",
      "\n",
      "Average test loss: 0.0677  Accuracy: 9802/10000 (98.02%)\n",
      "Evaluation time: 1.99 seconds\n",
      "\n",
      "Epoch: 45\n",
      "[    0/60000 (  0%)]  Loss: 0.0002  Batch time: 0.0232s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0017  Batch time: 0.0219s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0009  Batch time: 0.0215s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0022  Batch time: 0.0221s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0914  Batch time: 0.0248s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0407  Batch time: 0.0218s\n",
      "Epoch training time: 25.81 seconds\n",
      "\n",
      "Average test loss: 0.0674  Accuracy: 9845/10000 (98.45%)\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 46\n",
      "[    0/60000 (  0%)]  Loss: 0.0065  Batch time: 0.0227s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0078  Batch time: 0.0241s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0253  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.1010  Batch time: 0.0160s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0113  Batch time: 0.0232s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0014  Batch time: 0.0239s\n",
      "Epoch training time: 27.83 seconds\n",
      "\n",
      "Average test loss: 0.0687  Accuracy: 9823/10000 (98.23%)\n",
      "Evaluation time: 2.10 seconds\n",
      "\n",
      "Epoch: 47\n",
      "[    0/60000 (  0%)]  Loss: 0.0099  Batch time: 0.0243s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0019  Batch time: 0.0221s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0037  Batch time: 0.0236s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0032  Batch time: 0.0234s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0558  Batch time: 0.0240s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0109  Batch time: 0.0221s\n",
      "Epoch training time: 25.60 seconds\n",
      "\n",
      "Average test loss: 0.0675  Accuracy: 9809/10000 (98.09%)\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 48\n",
      "[    0/60000 (  0%)]  Loss: 0.0053  Batch time: 0.0220s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0113  Batch time: 0.0226s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0239  Batch time: 0.0219s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0130  Batch time: 0.0250s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0025  Batch time: 0.0328s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0021  Batch time: 0.0284s\n",
      "Epoch training time: 27.71 seconds\n",
      "\n",
      "Average test loss: 0.0639  Accuracy: 9840/10000 (98.40%)\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 49\n",
      "[    0/60000 (  0%)]  Loss: 0.0047  Batch time: 0.0262s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0016  Batch time: 0.0240s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0009  Batch time: 0.0273s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0003  Batch time: 0.0277s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0097  Batch time: 0.0267s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0332  Batch time: 0.0288s\n",
      "Epoch training time: 28.85 seconds\n",
      "\n",
      "Average test loss: 0.0616  Accuracy: 9844/10000 (98.44%)\n",
      "Evaluation time: 1.89 seconds\n",
      "\n",
      "Epoch: 50\n",
      "[    0/60000 (  0%)]  Loss: 0.0016  Batch time: 0.0265s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0031  Batch time: 0.0265s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0006  Batch time: 0.0212s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0023  Batch time: 0.0221s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0432  Batch time: 0.0212s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0258  Batch time: 0.0232s\n",
      "Epoch training time: 26.94 seconds\n",
      "\n",
      "Average test loss: 0.0781  Accuracy: 9811/10000 (98.11%)\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Execution time: 1433.98 seconds\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "train_loss_history, test_loss_history = [], []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "    evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.0781  Accuracy: 9811/10000 (98.11%)\n",
      "Evaluation time: 1.89 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.885340690612793"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Confusion matrix\n",
    "def plot_confusion_matrix(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())   # CPU로 변환 후 numpy()\n",
    "            all_labels.extend(target.numpy())        # target은 원래 CPU라 OK\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, loss_history):\n",
    "    model.eval()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_samples = len(data_loader.dataset)\n",
    "    correct_samples = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            output = F.log_softmax(model(data.to(device)), dim=1)\n",
    "            loss = F.nll_loss(output, target.to(device), reduction='sum')\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct_samples += pred.eq(target.to(device)).sum()\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    loss_history.append(avg_loss)\n",
    "    eval_time = time.time() - start_time\n",
    "\n",
    "    accuracy = (100.0 * correct_samples / total_samples).item()\n",
    "\n",
    "    print(f\"\\nAverage test loss: {avg_loss:.4f}  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Evaluation time: {eval_time:.2f} seconds\\n\")\n",
    "\n",
    "    return avg_loss, accuracy, eval_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_times = []\n",
    "# eval_times = []\n",
    "\n",
    "# for epoch in range(1, N_EPOCHS + 1):\n",
    "#     print('Epoch:', epoch)\n",
    "#     t = train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "#     e = evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "#     train_times.append(t)\n",
    "#     eval_times.append(e)\n",
    "\n",
    "# print(\"Total training time:\", sum(train_times))\n",
    "# print(\"Total evaluation time:\", sum(eval_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[    0/60000 (  0%)]  Loss: 0.0379  Batch time: 0.0243s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0052  Batch time: 0.0228s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0413  Batch time: 0.0232s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0017  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0405  Batch time: 0.0219s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0314  Batch time: 0.0225s\n",
      "Epoch training time: 25.49 seconds\n",
      "\n",
      "Average test loss: 0.0740  Accuracy: 98.18%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 2\n",
      "[    0/60000 (  0%)]  Loss: 0.0031  Batch time: 0.0226s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0032  Batch time: 0.0200s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0005  Batch time: 0.0199s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0021  Batch time: 0.0231s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0231  Batch time: 0.0223s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0006  Batch time: 0.0240s\n",
      "Epoch training time: 25.59 seconds\n",
      "\n",
      "Average test loss: 0.0674  Accuracy: 98.22%\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 3\n",
      "[    0/60000 (  0%)]  Loss: 0.0059  Batch time: 0.0227s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0002  Batch time: 0.0236s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0017  Batch time: 0.0239s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0041  Batch time: 0.0222s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0206  Batch time: 0.0198s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0043  Batch time: 0.0219s\n",
      "Epoch training time: 25.58 seconds\n",
      "\n",
      "Average test loss: 0.0718  Accuracy: 98.30%\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 4\n",
      "[    0/60000 (  0%)]  Loss: 0.0074  Batch time: 0.0232s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0015  Batch time: 0.0162s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0111  Batch time: 0.0252s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0067  Batch time: 0.0220s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0085  Batch time: 0.0227s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0016  Batch time: 0.0234s\n",
      "Epoch training time: 26.49 seconds\n",
      "\n",
      "Average test loss: 0.0599  Accuracy: 98.51%\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 5\n",
      "[    0/60000 (  0%)]  Loss: 0.0043  Batch time: 0.0226s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0162  Batch time: 0.0216s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0295  Batch time: 0.0245s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0005  Batch time: 0.0241s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0318  Batch time: 0.0226s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0040  Batch time: 0.0239s\n",
      "Epoch training time: 25.97 seconds\n",
      "\n",
      "Average test loss: 0.0667  Accuracy: 98.40%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 6\n",
      "[    0/60000 (  0%)]  Loss: 0.0001  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0117  Batch time: 0.0222s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0017  Batch time: 0.0227s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0004  Batch time: 0.0224s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0025  Batch time: 0.0214s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0078  Batch time: 0.0217s\n",
      "Epoch training time: 25.98 seconds\n",
      "\n",
      "Average test loss: 0.0862  Accuracy: 98.15%\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 7\n",
      "[    0/60000 (  0%)]  Loss: 0.0007  Batch time: 0.0230s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0028  Batch time: 0.0220s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0155  Batch time: 0.0219s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0050  Batch time: 0.0225s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0012  Batch time: 0.0236s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0013  Batch time: 0.0290s\n",
      "Epoch training time: 27.11 seconds\n",
      "\n",
      "Average test loss: 0.0733  Accuracy: 98.14%\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 8\n",
      "[    0/60000 (  0%)]  Loss: 0.0445  Batch time: 0.0251s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0086  Batch time: 0.0252s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0010  Batch time: 0.0201s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0016  Batch time: 0.0263s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0005  Batch time: 0.0228s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0610  Batch time: 0.0229s\n",
      "Epoch training time: 27.48 seconds\n",
      "\n",
      "Average test loss: 0.0671  Accuracy: 98.12%\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 9\n",
      "[    0/60000 (  0%)]  Loss: 0.0079  Batch time: 0.0244s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0005  Batch time: 0.0252s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0453  Batch time: 0.0230s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0218  Batch time: 0.0242s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0670  Batch time: 0.0223s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0006  Batch time: 0.0240s\n",
      "Epoch training time: 26.85 seconds\n",
      "\n",
      "Average test loss: 0.0799  Accuracy: 98.08%\n",
      "Evaluation time: 1.96 seconds\n",
      "\n",
      "Epoch: 10\n",
      "[    0/60000 (  0%)]  Loss: 0.0244  Batch time: 0.0237s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0016  Batch time: 0.0228s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0015  Batch time: 0.0240s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0012  Batch time: 0.0230s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0226  Batch time: 0.0144s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0217  Batch time: 0.0209s\n",
      "Epoch training time: 24.40 seconds\n",
      "\n",
      "Average test loss: 0.0705  Accuracy: 98.32%\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 11\n",
      "[    0/60000 (  0%)]  Loss: 0.0001  Batch time: 0.0228s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0040  Batch time: 0.0245s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0125  Batch time: 0.0238s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0252  Batch time: 0.0221s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0005  Batch time: 0.0230s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0016  Batch time: 0.0251s\n",
      "Epoch training time: 27.20 seconds\n",
      "\n",
      "Average test loss: 0.0623  Accuracy: 98.39%\n",
      "Evaluation time: 2.29 seconds\n",
      "\n",
      "Epoch: 12\n",
      "[    0/60000 (  0%)]  Loss: 0.0004  Batch time: 0.0218s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0011  Batch time: 0.0242s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0019  Batch time: 0.0233s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0124  Batch time: 0.0209s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0052  Batch time: 0.0235s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0040  Batch time: 0.0224s\n",
      "Epoch training time: 28.43 seconds\n",
      "\n",
      "Average test loss: 0.0677  Accuracy: 98.43%\n",
      "Evaluation time: 1.96 seconds\n",
      "\n",
      "Epoch: 13\n",
      "[    0/60000 (  0%)]  Loss: 0.0002  Batch time: 0.0240s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0090  Batch time: 0.0236s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0008  Batch time: 0.0238s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0410  Batch time: 0.0225s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0213  Batch time: 0.0232s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0049  Batch time: 0.0237s\n",
      "Epoch training time: 28.14 seconds\n",
      "\n",
      "Average test loss: 0.0654  Accuracy: 98.28%\n",
      "Evaluation time: 2.03 seconds\n",
      "\n",
      "Epoch: 14\n",
      "[    0/60000 (  0%)]  Loss: 0.0029  Batch time: 0.0270s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0035  Batch time: 0.0242s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0055  Batch time: 0.0230s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0016  Batch time: 0.0230s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0025  Batch time: 0.0214s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0039  Batch time: 0.0207s\n",
      "Epoch training time: 27.51 seconds\n",
      "\n",
      "Average test loss: 0.0629  Accuracy: 98.59%\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 15\n",
      "[    0/60000 (  0%)]  Loss: 0.0056  Batch time: 0.0230s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0082  Batch time: 0.0203s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0007  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0014  Batch time: 0.0236s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0020  Batch time: 0.0222s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0056  Batch time: 0.0258s\n",
      "Epoch training time: 27.28 seconds\n",
      "\n",
      "Average test loss: 0.0693  Accuracy: 98.39%\n",
      "Evaluation time: 2.06 seconds\n",
      "\n",
      "Epoch: 16\n",
      "[    0/60000 (  0%)]  Loss: 0.0004  Batch time: 0.0260s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0412  Batch time: 0.0245s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0036  Batch time: 0.0258s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0040  Batch time: 0.0267s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0002  Batch time: 0.0254s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0246  Batch time: 0.0233s\n",
      "Epoch training time: 27.89 seconds\n",
      "\n",
      "Average test loss: 0.0694  Accuracy: 98.19%\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 17\n",
      "[    0/60000 (  0%)]  Loss: 0.0016  Batch time: 0.0237s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0876  Batch time: 0.0222s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0209  Batch time: 0.0227s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0007  Batch time: 0.0204s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0101  Batch time: 0.0226s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0018  Batch time: 0.0240s\n",
      "Epoch training time: 26.51 seconds\n",
      "\n",
      "Average test loss: 0.0641  Accuracy: 98.47%\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 18\n",
      "[    0/60000 (  0%)]  Loss: 0.0006  Batch time: 0.0284s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0009  Batch time: 0.0285s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0224  Batch time: 0.0253s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0001  Batch time: 0.0237s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0005  Batch time: 0.0222s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0072  Batch time: 0.0214s\n",
      "Epoch training time: 27.75 seconds\n",
      "\n",
      "Average test loss: 0.0771  Accuracy: 98.18%\n",
      "Evaluation time: 1.96 seconds\n",
      "\n",
      "Epoch: 19\n",
      "[    0/60000 (  0%)]  Loss: 0.0181  Batch time: 0.0275s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0126  Batch time: 0.0223s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0377  Batch time: 0.0225s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0006  Batch time: 0.0230s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0314  Batch time: 0.0216s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0029  Batch time: 0.0229s\n",
      "Epoch training time: 27.46 seconds\n",
      "\n",
      "Average test loss: 0.0566  Accuracy: 98.59%\n",
      "Evaluation time: 2.32 seconds\n",
      "\n",
      "Epoch: 20\n",
      "[    0/60000 (  0%)]  Loss: 0.0015  Batch time: 0.0253s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0222  Batch time: 0.0270s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0028  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0010  Batch time: 0.0220s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0002  Batch time: 0.0241s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0112  Batch time: 0.0238s\n",
      "Epoch training time: 27.96 seconds\n",
      "\n",
      "Average test loss: 0.0611  Accuracy: 98.47%\n",
      "Evaluation time: 2.12 seconds\n",
      "\n",
      "Epoch: 21\n",
      "[    0/60000 (  0%)]  Loss: 0.0006  Batch time: 0.0232s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0145  Batch time: 0.0240s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0003  Batch time: 0.0236s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0022  Batch time: 0.0217s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0064  Batch time: 0.0239s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0017  Batch time: 0.0227s\n",
      "Epoch training time: 26.73 seconds\n",
      "\n",
      "Average test loss: 0.0754  Accuracy: 98.30%\n",
      "Evaluation time: 1.97 seconds\n",
      "\n",
      "Epoch: 22\n",
      "[    0/60000 (  0%)]  Loss: 0.0442  Batch time: 0.0228s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0247  Batch time: 0.0233s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0138  Batch time: 0.0195s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0070  Batch time: 0.0234s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0052  Batch time: 0.0228s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0009  Batch time: 0.0244s\n",
      "Epoch training time: 26.46 seconds\n",
      "\n",
      "Average test loss: 0.0593  Accuracy: 98.68%\n",
      "Evaluation time: 1.96 seconds\n",
      "\n",
      "Epoch: 23\n",
      "[    0/60000 (  0%)]  Loss: 0.0019  Batch time: 0.0244s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0016  Batch time: 0.0229s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0013  Batch time: 0.0245s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0009  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0060  Batch time: 0.0242s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0050  Batch time: 0.0234s\n",
      "Epoch training time: 26.98 seconds\n",
      "\n",
      "Average test loss: 0.0633  Accuracy: 98.59%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 24\n",
      "[    0/60000 (  0%)]  Loss: 0.0751  Batch time: 0.0277s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0001  Batch time: 0.0273s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0001  Batch time: 0.0274s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0032  Batch time: 0.0264s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0640  Batch time: 0.0263s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0028  Batch time: 0.0226s\n",
      "Epoch training time: 29.59 seconds\n",
      "\n",
      "Average test loss: 0.0805  Accuracy: 97.95%\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 25\n",
      "[    0/60000 (  0%)]  Loss: 0.0422  Batch time: 0.0277s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0071  Batch time: 0.0264s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0166  Batch time: 0.0244s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0012  Batch time: 0.0265s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0373  Batch time: 0.0256s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0004  Batch time: 0.0263s\n",
      "Epoch training time: 28.14 seconds\n",
      "\n",
      "Average test loss: 0.0769  Accuracy: 98.44%\n",
      "Evaluation time: 2.03 seconds\n",
      "\n",
      "Epoch: 26\n",
      "[    0/60000 (  0%)]  Loss: 0.0002  Batch time: 0.0274s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0411  Batch time: 0.0271s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0051  Batch time: 0.0270s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0113  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0012  Batch time: 0.0238s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0032  Batch time: 0.0231s\n",
      "Epoch training time: 27.83 seconds\n",
      "\n",
      "Average test loss: 0.0724  Accuracy: 98.39%\n",
      "Evaluation time: 2.04 seconds\n",
      "\n",
      "Epoch: 27\n",
      "[    0/60000 (  0%)]  Loss: 0.0272  Batch time: 0.0181s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0183  Batch time: 0.0227s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0049  Batch time: 0.0222s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0068  Batch time: 0.0280s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0008  Batch time: 0.0228s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0005  Batch time: 0.0215s\n",
      "Epoch training time: 27.58 seconds\n",
      "\n",
      "Average test loss: 0.0673  Accuracy: 98.58%\n",
      "Evaluation time: 2.33 seconds\n",
      "\n",
      "Epoch: 28\n",
      "[    0/60000 (  0%)]  Loss: 0.0584  Batch time: 0.0250s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0820  Batch time: 0.0249s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0008  Batch time: 0.0276s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0262  Batch time: 0.0273s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0014  Batch time: 0.0240s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0073  Batch time: 0.0226s\n",
      "Epoch training time: 27.53 seconds\n",
      "\n",
      "Average test loss: 0.0922  Accuracy: 98.06%\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 29\n",
      "[    0/60000 (  0%)]  Loss: 0.0025  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0013  Batch time: 0.0227s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0273  Batch time: 0.0197s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0019  Batch time: 0.0218s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0013  Batch time: 0.0233s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0086  Batch time: 0.0218s\n",
      "Epoch training time: 26.11 seconds\n",
      "\n",
      "Average test loss: 0.0606  Accuracy: 98.39%\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 30\n",
      "[    0/60000 (  0%)]  Loss: 0.0043  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0073  Batch time: 0.0240s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0170  Batch time: 0.0236s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0186  Batch time: 0.0201s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0008  Batch time: 0.0208s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0010  Batch time: 0.0229s\n",
      "Epoch training time: 25.88 seconds\n",
      "\n",
      "Average test loss: 0.0591  Accuracy: 98.67%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 31\n",
      "[    0/60000 (  0%)]  Loss: 0.0004  Batch time: 0.0234s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0027  Batch time: 0.0237s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0398  Batch time: 0.0236s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0021  Batch time: 0.0232s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0306  Batch time: 0.0213s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0244  Batch time: 0.0216s\n",
      "Epoch training time: 26.18 seconds\n",
      "\n",
      "Average test loss: 0.0790  Accuracy: 98.17%\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 32\n",
      "[    0/60000 (  0%)]  Loss: 0.0340  Batch time: 0.0231s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0004  Batch time: 0.0241s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0011  Batch time: 0.0217s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0303  Batch time: 0.0220s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0003  Batch time: 0.0222s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0290  Batch time: 0.0218s\n",
      "Epoch training time: 26.44 seconds\n",
      "\n",
      "Average test loss: 0.0636  Accuracy: 98.56%\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 33\n",
      "[    0/60000 (  0%)]  Loss: 0.0118  Batch time: 0.0233s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0010  Batch time: 0.0234s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0239  Batch time: 0.0227s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0001  Batch time: 0.0229s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0003  Batch time: 0.0243s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0268  Batch time: 0.0284s\n",
      "Epoch training time: 26.66 seconds\n",
      "\n",
      "Average test loss: 0.0664  Accuracy: 98.59%\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 34\n",
      "[    0/60000 (  0%)]  Loss: 0.0001  Batch time: 0.0268s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0157  Batch time: 0.0270s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0003  Batch time: 0.0263s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0002  Batch time: 0.0263s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0002  Batch time: 0.0278s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0058  Batch time: 0.0282s\n",
      "Epoch training time: 29.36 seconds\n",
      "\n",
      "Average test loss: 0.0844  Accuracy: 98.21%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 35\n",
      "[    0/60000 (  0%)]  Loss: 0.0794  Batch time: 0.0225s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0021  Batch time: 0.0204s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0031  Batch time: 0.0224s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0020  Batch time: 0.0237s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0005  Batch time: 0.0235s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0071  Batch time: 0.0225s\n",
      "Epoch training time: 26.15 seconds\n",
      "\n",
      "Average test loss: 0.0748  Accuracy: 98.32%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 36\n",
      "[    0/60000 (  0%)]  Loss: 0.0014  Batch time: 0.0230s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0004  Batch time: 0.0226s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0155  Batch time: 0.0235s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0150  Batch time: 0.0219s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0000  Batch time: 0.0255s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0146  Batch time: 0.0221s\n",
      "Epoch training time: 27.21 seconds\n",
      "\n",
      "Average test loss: 0.0704  Accuracy: 98.57%\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 37\n",
      "[    0/60000 (  0%)]  Loss: 0.0016  Batch time: 0.0231s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0002  Batch time: 0.0236s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0046  Batch time: 0.0230s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0004  Batch time: 0.0233s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0032  Batch time: 0.0238s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0374  Batch time: 0.0249s\n",
      "Epoch training time: 27.79 seconds\n",
      "\n",
      "Average test loss: 0.0861  Accuracy: 97.81%\n",
      "Evaluation time: 2.26 seconds\n",
      "\n",
      "Epoch: 38\n",
      "[    0/60000 (  0%)]  Loss: 0.0015  Batch time: 0.0209s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0013  Batch time: 0.0219s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0014  Batch time: 0.0203s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0083  Batch time: 0.0231s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0035  Batch time: 0.0232s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0019  Batch time: 0.0236s\n",
      "Epoch training time: 26.71 seconds\n",
      "\n",
      "Average test loss: 0.0717  Accuracy: 98.40%\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 39\n",
      "[    0/60000 (  0%)]  Loss: 0.0009  Batch time: 0.0226s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0115  Batch time: 0.0247s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0036  Batch time: 0.0232s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0003  Batch time: 0.0230s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0002  Batch time: 0.0219s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0070  Batch time: 0.0261s\n",
      "Epoch training time: 26.42 seconds\n",
      "\n",
      "Average test loss: 0.0720  Accuracy: 98.43%\n",
      "Evaluation time: 1.93 seconds\n",
      "\n",
      "Epoch: 40\n",
      "[    0/60000 (  0%)]  Loss: 0.0015  Batch time: 0.0230s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0011  Batch time: 0.0223s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0003  Batch time: 0.0259s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0326  Batch time: 0.0205s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0008  Batch time: 0.0221s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0050  Batch time: 0.0228s\n",
      "Epoch training time: 26.22 seconds\n",
      "\n",
      "Average test loss: 0.0937  Accuracy: 97.79%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 41\n",
      "[    0/60000 (  0%)]  Loss: 0.0008  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0061  Batch time: 0.0243s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0528  Batch time: 0.0238s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0064  Batch time: 0.0256s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0008  Batch time: 0.0228s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0011  Batch time: 0.0235s\n",
      "Epoch training time: 26.40 seconds\n",
      "\n",
      "Average test loss: 0.0691  Accuracy: 98.46%\n",
      "Evaluation time: 1.91 seconds\n",
      "\n",
      "Epoch: 42\n",
      "[    0/60000 (  0%)]  Loss: 0.0006  Batch time: 0.0236s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0000  Batch time: 0.0210s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0002  Batch time: 0.0232s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0007  Batch time: 0.0235s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0010  Batch time: 0.0237s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0006  Batch time: 0.0226s\n",
      "Epoch training time: 26.68 seconds\n",
      "\n",
      "Average test loss: 0.0643  Accuracy: 98.63%\n",
      "Evaluation time: 1.95 seconds\n",
      "\n",
      "Epoch: 43\n",
      "[    0/60000 (  0%)]  Loss: 0.0003  Batch time: 0.0230s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0049  Batch time: 0.0233s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0002  Batch time: 0.0228s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0004  Batch time: 0.0226s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0006  Batch time: 0.0247s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0136  Batch time: 0.0262s\n",
      "Epoch training time: 26.50 seconds\n",
      "\n",
      "Average test loss: 0.0763  Accuracy: 98.29%\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 44\n",
      "[    0/60000 (  0%)]  Loss: 0.0081  Batch time: 0.0221s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0019  Batch time: 0.0221s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0039  Batch time: 0.0255s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0174  Batch time: 0.0226s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0588  Batch time: 0.0238s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0014  Batch time: 0.0226s\n",
      "Epoch training time: 26.57 seconds\n",
      "\n",
      "Average test loss: 0.0847  Accuracy: 98.23%\n",
      "Evaluation time: 2.04 seconds\n",
      "\n",
      "Epoch: 45\n",
      "[    0/60000 (  0%)]  Loss: 0.0067  Batch time: 0.0238s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0002  Batch time: 0.0237s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0019  Batch time: 0.0231s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0023  Batch time: 0.0250s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0100  Batch time: 0.0236s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0014  Batch time: 0.0196s\n",
      "Epoch training time: 26.64 seconds\n",
      "\n",
      "Average test loss: 0.0613  Accuracy: 98.51%\n",
      "Evaluation time: 1.94 seconds\n",
      "\n",
      "Epoch: 46\n",
      "[    0/60000 (  0%)]  Loss: 0.0013  Batch time: 0.0225s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0003  Batch time: 0.0239s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0025  Batch time: 0.0217s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0001  Batch time: 0.0231s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0014  Batch time: 0.0263s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0043  Batch time: 0.0252s\n",
      "Epoch training time: 27.78 seconds\n",
      "\n",
      "Average test loss: 0.0709  Accuracy: 98.51%\n",
      "Evaluation time: 2.31 seconds\n",
      "\n",
      "Epoch: 47\n",
      "[    0/60000 (  0%)]  Loss: 0.0004  Batch time: 0.0229s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0000  Batch time: 0.0245s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0002  Batch time: 0.0241s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0029  Batch time: 0.0242s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0018  Batch time: 0.0259s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0120  Batch time: 0.0240s\n",
      "Epoch training time: 29.78 seconds\n",
      "\n",
      "Average test loss: 0.0803  Accuracy: 98.01%\n",
      "Evaluation time: 2.32 seconds\n",
      "\n",
      "Epoch: 48\n",
      "[    0/60000 (  0%)]  Loss: 0.0061  Batch time: 0.0247s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0065  Batch time: 0.0261s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0667  Batch time: 0.0227s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0394  Batch time: 0.0225s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0006  Batch time: 0.0198s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0004  Batch time: 0.0216s\n",
      "Epoch training time: 27.01 seconds\n",
      "\n",
      "Average test loss: 0.0738  Accuracy: 98.44%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Epoch: 49\n",
      "[    0/60000 (  0%)]  Loss: 0.0008  Batch time: 0.0234s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0003  Batch time: 0.0241s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0201  Batch time: 0.0212s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0030  Batch time: 0.0224s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0209  Batch time: 0.0228s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0075  Batch time: 0.0279s\n",
      "Epoch training time: 27.14 seconds\n",
      "\n",
      "Average test loss: 0.0683  Accuracy: 98.48%\n",
      "Evaluation time: 1.90 seconds\n",
      "\n",
      "Epoch: 50\n",
      "[    0/60000 (  0%)]  Loss: 0.0063  Batch time: 0.0263s\n",
      "[10000/60000 ( 17%)]  Loss: 0.0053  Batch time: 0.0256s\n",
      "[20000/60000 ( 33%)]  Loss: 0.0014  Batch time: 0.0282s\n",
      "[30000/60000 ( 50%)]  Loss: 0.0156  Batch time: 0.0218s\n",
      "[40000/60000 ( 67%)]  Loss: 0.0003  Batch time: 0.0223s\n",
      "[50000/60000 ( 83%)]  Loss: 0.0001  Batch time: 0.0222s\n",
      "Epoch training time: 27.64 seconds\n",
      "\n",
      "Average test loss: 0.0684  Accuracy: 98.44%\n",
      "Evaluation time: 1.92 seconds\n",
      "\n",
      "Total training time: 1351.1582028865814\n",
      "Total evaluation time: 99.72662115097046\n"
     ]
    }
   ],
   "source": [
    "train_times = []\n",
    "eval_times = []\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    print('Epoch:', epoch)\n",
    "\n",
    "    train_time = train_epoch(model, optimizer, train_loader, train_loss_history)\n",
    "\n",
    "    avg_loss, accuracy, eval_time = evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "    train_times.append(train_time)\n",
    "    eval_times.append(eval_time)\n",
    "\n",
    "print(\"Total training time:\", sum(train_times))\n",
    "print(\"Total evaluation time:\", sum(eval_times))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ViT train time: 1351.1582028865814\n"
     ]
    }
   ],
   "source": [
    "vit_train_time = sum(train_times)\n",
    "print(\"Total ViT train time:\", vit_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average test loss: 0.0684  Accuracy: 98.44%\n",
      "Evaluation time: 1.93 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ViT-only 최종 Accuracy\n",
    "vit_loss, vit_acc, vit_eval_time = evaluate(model, test_loader, test_loss_history)\n",
    "\n",
    "# ViT-only FLOPs\n",
    "vit_flops, vit_params = compute_flops(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize correct and wrong predictions\n",
    "def plot_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_images = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data_gpu = data.to(device)\n",
    "\n",
    "            output = model(data_gpu)\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())   # CPU로 변환 후 numpy()\n",
    "            all_labels.extend(target.numpy())       # target은 CPU에 원래 있음\n",
    "            all_images.extend(data.numpy())         # 시각화는 CPU 데이터 사용\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_images = np.array(all_images)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(all_images[i][0], cmap='gray')\n",
    "        ax.set_title(f'Label: {all_labels[i]}\\nPred: {all_preds[i]}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(model_name, acc, loss, train_time, eval_time, flops, params):\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"accuracy\": acc,\n",
    "        \"loss\": loss,\n",
    "        \"train_time\": train_time,\n",
    "        \"eval_time\": eval_time,\n",
    "        \"flops\": flops,\n",
    "        \"params\": params\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "\n",
    "def compute_flops(model, img_size=28):\n",
    "    dummy = torch.randn(1, 1, img_size, img_size).to(device)\n",
    "    macs, params = profile(model, inputs=(dummy,), verbose=False)\n",
    "    return macs, params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLViT_FLOPs(nn.Module):\n",
    "    def __init__(self, vit_model, mask):\n",
    "        super().__init__()\n",
    "        self.vit = vit_model\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 임베딩\n",
    "        x = self.vit.to_patch_embedding(x)\n",
    "        pe = posemb_sincos_2d(x).to(x.device)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "\n",
    "        # 2. Mask 적용\n",
    "        x = x[:, self.mask, :]\n",
    "\n",
    "        # 3. Transformer 적용\n",
    "        x = self.vit.transformer(x)\n",
    "\n",
    "        # 4. 평균 풀링\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # 5. Classification head\n",
    "        return self.vit.linear_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = torch.tensor(ViTnet.get_patches()).bool()\n",
    "\n",
    "wrapper = RLViT_FLOPs(ViTnet, final_mask).to(device)\n",
    "rl_flops, rl_params = compute_flops(wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL-ViT FLOPs: 102718464.0\n",
      "RL-ViT Params: 3943082.0\n"
     ]
    }
   ],
   "source": [
    "# RL 학습 끝나고 mask 불러오기\n",
    "final_mask = torch.tensor(ViTnet.get_patches()).bool()\n",
    "\n",
    "# FLOPs 측정용 wrapper 모델 생성\n",
    "wrapper = RLViT_FLOPs(ViTnet, final_mask).to(device)\n",
    "\n",
    "rl_flops, rl_params = compute_flops(wrapper)\n",
    "\n",
    "print(\"RL-ViT FLOPs:\", rl_flops)\n",
    "print(\"RL-ViT Params:\", rl_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(vit_res, rl_res):\n",
    "    print(\"\\n==================== RESULTS COMPARISON ====================\")\n",
    "    print(f\"ViT Accuracy:      {vit_res['accuracy']:.2f}%\")\n",
    "    print(f\"RL-ViT Accuracy:   {rl_res['accuracy']:.2f}%\")\n",
    "\n",
    "    print(f\"ViT FLOPs:         {vit_res['flops']:,}\")\n",
    "    print(f\"RL-ViT FLOPs:      {rl_res['flops']:,}\")\n",
    "\n",
    "    print(f\"ViT Params:        {vit_res['params']:,}\")\n",
    "    print(f\"RL-ViT Params:     {rl_res['params']:,}\")\n",
    "\n",
    "    print(f\"Train Time (ViT):  {vit_res['train_time']:.2f}s\")\n",
    "    print(f\"Train Time (RL):   {rl_res['train_time']:.2f}s\")\n",
    "\n",
    "    print(f\"Speedup (FLOPs RL/ViT): {rl_res['flops'] / vit_res['flops']:.3f}x\")\n",
    "    print(\"============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_curves(vit_acc_list, rl_acc_list):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(vit_acc_list, label=\"ViT Accuracy\", linewidth=2)\n",
    "    plt.plot(rl_acc_list, label=\"RL-ViT Accuracy\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(vit_loss_list, rl_loss_list):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(vit_loss_list, label=\"ViT Loss\", linewidth=2)\n",
    "    plt.plot(rl_loss_list, label=\"RL-ViT Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_curves(vit_times, rl_times):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(vit_times, label=\"ViT Train Time\", linewidth=2)\n",
    "    plt.plot(rl_times, label=\"RL-ViT Train Time\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Seconds\")\n",
    "    plt.title(\"Training Time per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flops_bar(vit_flops, rl_flops):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.bar([\"ViT\", \"RL-ViT\"], [vit_flops, rl_flops], color=[\"skyblue\", \"orange\"])\n",
    "    plt.ylabel(\"FLOPs\")\n",
    "    plt.title(\"FLOPs Comparison\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(vit_res, rl_res,\n",
    "                         vit_acc_list, rl_acc_list,\n",
    "                         vit_loss_list, rl_loss_list,\n",
    "                         vit_times, rl_times):\n",
    "\n",
    "    print_comparison(vit_res, rl_res)\n",
    "\n",
    "    plot_accuracy_curves(vit_acc_list, rl_acc_list)\n",
    "    plot_loss_curves(vit_loss_list, rl_loss_list)\n",
    "    plot_time_curves(vit_times, rl_times)\n",
    "    plot_flops_bar(vit_res[\"flops\"], rl_res[\"flops\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== RESULTS COMPARISON ====================\n",
      "ViT Accuracy:      98.44%\n",
      "RL-ViT Accuracy:   97.62%\n",
      "ViT FLOPs:         24,959,472.0\n",
      "RL-ViT FLOPs:      102,718,464.0\n",
      "ViT Params:        496,458.0\n",
      "RL-ViT Params:     3,943,082.0\n",
      "Train Time (ViT):  1351.16s\n",
      "Train Time (RL):   886.65s\n",
      "Speedup (FLOPs RL/ViT): 4.115x\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "vit_res = {\n",
    "    \"accuracy\": vit_acc,\n",
    "    \"flops\": vit_flops,\n",
    "    \"params\": vit_params,\n",
    "    \"train_time\": vit_train_time\n",
    "}\n",
    "\n",
    "rl_res = {\n",
    "    \"accuracy\": rl_acc,\n",
    "    \"flops\": rl_flops,\n",
    "    \"params\": rl_params,\n",
    "    \"train_time\": rl_train_time\n",
    "}\n",
    "\n",
    "print_comparison(vit_res, rl_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rXC93PvZHALw",
    "-WOxO6GNga2i",
    "ng75uqlIgCvl",
    "YL2DwKZ1Mdkr",
    "dDu_n6NZDqEs",
    "Nql4-HUNB9YC",
    "7S_pihMq-XeA",
    "S9IYQ9yHAUwe",
    "akTharX5RaL8",
    "9VY2VipXRejU"
   ],
   "provenance": [
    {
     "file_id": "19aOvDZhUQ6-Z6twxhyqRHIrfk9R31FP-",
     "timestamp": 1692296683556
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
